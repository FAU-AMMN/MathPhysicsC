\chapter{Vektoranalysis}
\label{\detokenize{vektoranalysis/vektoranalysis:vektoranalysis}}\label{\detokenize{vektoranalysis/vektoranalysis::doc}}
\par
In diesem Kapitel der Vorlesung führen wir wichtige Konzepte der \emph{Vektoranalysis} ein.
Insbesondere schaffen wir die mathematischen Grundlagen für eine spezielle Art der mehrdimensionalen Integration, das Integrieren über sogenannte \emph{Untermannigfaltigkeiten} des \(\R^n\).
Um diese Integration durchführen zu können, entwickeln wir das Kalkül der \emph{Differentialformen} auf Mannigfaltigkeiten.

\par
Dieses Kalkül lässt auch den geometrischen Gehalt physikalischer Theorien wie Elektrodynamik oder Allgemeine Relativitätstheorie klar hervortreten.
So lassen sich beispielsweise die Maxwellschen Gleichungen der Elektrodynamik mit Hilfe des Differentialformenkalkül elegant beschreiben.

\par
Als zusätzliche Literatur und Referenz für diese Thematiken empfehlen wir das Buch von Agricola und Friedrich \cite{AF13}.


\section{Multilinearformen}
\label{\detokenize{vektoranalysis/multilinear:multilinearformen}}\label{\detokenize{vektoranalysis/multilinear::doc}}
\par
In diesem Abschnitt wollen wir die Definition der sogenannten \emph{Multilinearformen} einführen.
Für beliebige Vektorräume \(\V, W\) über einem Körper \(\K\) haben Sie bereits den Begriff der \emph{Linearform}, also einer linearen Abbildung \(\varphi:\V\rightarrow W\) kennengelernt.
Die Idee der Multilinearform ist anstatt nur einem, gleich \(k\) viele Vektorräume \(V_1,\ldots,V_k\) für \(k \in \N\) über \(\K\) zu betrachten und das Konzept der Linearität auf eine Abbildung \(\varphi:\V_1\times\ldots\V_k\rightarrow W\) zu übertragen.

\par
Zur Vereinfachung werden wir im Folgenden nur den Körper \(\K=\R\) betrachten, in den meisten Fällen lassen sich die hier beschriebenen Konzepte aber direkt auf allgemeine Körper übertragen.
Wir beginnen zunächst mit einer Wiederholung und betrachten die schon bekannten Linearformen.
Insbesondere soll der nächste Abschnitt die verschiedenen Begriffe des Dualraums abgrenzen.


\subsection{Dualräume}
\label{\detokenize{vektoranalysis/multilinear:dualraume}}
\par
Für einen reellen Vektorraum \(\V\) wollen wir lineare Abbildungen \(\varphi:V\to\R\) betrachten.
Diese lassen sich mit Hilfe der folgenden Definition zum algebraischen Dualraum zusammenfassen.
\begin{definition}{(Algebraischer Dualraum)}{vektoranalysis/multilinear:def:algebraischerDualraum}



\par
Es sei \(\V\) ein beliebiger \(\R\) Vektorraum.
Dann nennen wir die Menge
\begin{align*}
\V^\ast := \{\varphi:\V\rightarrow\R: \varphi\text{ ist linear}\}
\end{align*}
\par
den \textbf{algebraischer Dualraum} zu \(V\).
\end{definition}

\par
Aus \cite{Ten21} ist bereits der Begriff des \emph{topologischen Dualraums} bekannt, welcher allerdings eine etwas restriktivere Definition hat.
Sie fordert nämlich noch zusätzlich die Stetigkeit der linearen Abbildungen.
\begin{definition}{(Topologischer Dualraum)}{vektoranalysis/multilinear:def:topologischerDualraum}



\par
Es sei \(\V\) ein normierter \(\R\) Vektorraum für einen Körper \(\R\).
Dann nennen wir die Menge
\begin{align*}
\V^\prime := \{\varphi:\V\rightarrow\R: \varphi\text{ ist linear und stetig}\}
\end{align*}
\par
den \textbf{topologischer Dualraum} zu \(V\).
\end{definition}

\begin{emphBox}{}{}
\par
Der algebraische Dualraum ist im Allgemeinen nicht gleich dem topologischen Dualraum.
Der Hauptzweck dieses Abschnitts ist es diese Tatsache klar zu machen und die Unterschiede der beiden Definitionen herauszustellen.
\end{emphBox}

\par
Der Integraloperator ist ein typisches Beispiel für einen linearen stetigen Operator.
\begin{example}{(Integraloperator)}{vektoranalysis/multilinear:example-2}



\par
Es sei \(\V := C([0,1])\) der Funktionenraum der stetigen Funktionen auf dem Intervall \([0,1] \subset \R\).
Dann ist der durch \(T \colon C([0,1]) \rightarrow \R\) definierte Integraloperator mit
\begin{align*}
T(f) := \int_0^1 f(x) \, \mathrm{d}x
\end{align*}
\par
ein Element des \emph{topologischen Dualraums}, d.h. \(T \in \V^\prime\), da man zeigen kann, dass er linear und stetig ist.
\end{example}

\par
Folgende Bemerkung sagt etwas über die minimale Struktur, die der Vektorraum \(V\) haben muss, damit die Definition des topologischen Dualraums sinnvoll ist.
\begin{remark}{}{vektoranalysis/multilinear:remark-3}



\par
Damit die \cref{vektoranalysis/multilinear:def:topologischerDualraum} sinnvoll ist, ist es in der Tat nicht notwendig, dass \(V\) ein normierter Raum ist. Es reicht anzunehmen, dass \(\V\) ein \emph{topologischer Vektorraum} ist.
\end{remark}

\par
Durch Vergleichen von \cref{vektoranalysis/multilinear:def:algebraischerDualraum} und \cref{vektoranalysis/multilinear:def:topologischerDualraum} erkennt man sofort, dass stets \(\V^\prime\subset \V^\ast\) gilt.
Außerdem stellt man fest, dass die beiden Räume im endlich dimensionalen Fall überein stimmen, wie folgendes Lemma aussagt.
\begin{lemma}{}{vektoranalysis/multilinear:lemma-4}



\par
Für \(n\in\N\) sei \(\V\) ein \(n\) dimensionaler \(\R\) Vektorraum, dessen Norm durch das Standardskalarprodukt induziert ist.
Dann gilt
\begin{align*}
V^\prime = V^\ast.
\end{align*}\end{lemma}

\begin{proof}
 In der Hausaufgabe zu zeigen.
\end{proof}

\par
Das folgende Beispiel aus der Funktionalanalysis erklärt, dass die Gleichheit von algebraischen und topologischen Dualräumen nicht mehr in unendlich dimensionalen Räumen gilt.
\begin{example}{(Differentialoperator)}{vektoranalysis/multilinear:example-5}



\par
Sei \(\V := C^1([0,1])\) der Vektorraum der stetig differenzierbaren Funktionen auf dem Intervall \([0,1] \subset \R\).
Wir betrachten im Folgenden den \emph{Differentialoperator}
\begin{align*}
D \colon V &\rightarrow \R \\
(Df)(x) &\mapsto f'(x), \quad \forall x \in [0,1].
\end{align*}
\par
Bekanntermaßen ist der Differentialoperator \(D\) \textbf{linear} und ist somit ein Element des algebraischen Dualraums, d.h., \(D \in V^\ast\).
Statten wir den Vektorraum \(C^1([0,1])\) mit der \emph{Supremumsnorm}
\begin{align*}
||f||_\infty := \sup_{x \in [0,1]} |f(x)|
\end{align*}
\par
aus und betrachten die Funktionenfolge \(f_n(x) := x^n\), dann sehen wir ein, dass die Supremumsnorm der Folge konstant ist mit \(||f_n||_\infty \equiv 1\) für alle \(n\in\N\).
Für den Differentialoperator \(D\) gilt jedoch
\begin{align*}
||Df_n||_\infty = \sup_{x \in [0,1]} |(Df_n)(x)| = \sup_{x \in [0,1]} |f_n'(x)| = \sup_{x \in [0,1]} |nx^{n-1}| = n.
\end{align*}
\par
Um die \emph{Stetigkeit} des Differentialoperators zu untersuchen betrachten wir die konstante Nullfunktion \(F_0 \in V\) mit \(F_0(x) \equiv 0\) für alle \(x \in [0,1]\).
Vergleichen wir nun den Abstand der konstanten Nullfunktionen zum ersten Folgenglied \(f_1\) unserer Funktionenfolge, so erhalten wir erwartungsgemäß
\begin{align*}
||f_1 - F_0||_\infty = ||f_1||_\infty = ||x^1||_\infty = 1 < \frac{3}{2} =: \delta.
\end{align*}
\par
Für den Differenzialoperator erhalten wir analog
\begin{align*}
||Df_1 - DF_0||_\infty = ||Df_1||_\infty = ||1||\infty < \frac{3}{2} =: \epsilon.
\end{align*}
\par
Wäre der Differenzialoperator \(D\) stetig, so müsste nach dem \(\epsilon-\delta\) Kriterium nun für jedes Folgenglied \(f_n\) unserer Funktionenfolge \(||Df_n - DF_0|| < \epsilon\) gelten, da der Abstand kleiner \(\delta\) ist wegen
\begin{align*}
||f_n - F_0||_\infty = ||f_n||_\infty = ||x^n||_\infty = 1 < \delta.
\end{align*}
\par
Jedoch sehen wir, dass die Folge der Ableitungen divergiert, d.h.,
\begin{align*}
||Df_n - DF_0||_\infty = ||Df_n||_\infty = ||nx^{n-1}||_\infty = n > \epsilon \quad \text{für } n\geq 2.
\end{align*}
\par
Wir sehen also ein, dass der Differentialoperator \textbf{nicht stetig} ist und somit kein Element des topologischen Dualraums \(V'\) sein kann.
Damit haben wir gezeigt, dass in unendlich dimensionalen Räumen \(V' \subsetneq V^\ast\) gilt.
\end{example}


\subsection{k Multilinearformen}
\label{\detokenize{vektoranalysis/multilinear:k-multilinearformen}}
\par
Nachdem wir uns den Begriff der Linearität ins Gedächtnis zurückgerufen haben und Dualräume erklärt haben, wollen wir was Konzept linearer Abbildungen in der folgenden Definition verallgemeinern.
\begin{definition}{(k Multilinearität)}{vektoranalysis/multilinear:def:multilinear}



\par
Sei \(k \in \N\) und es seien \(\V_i, i=1,\ldots,k\), sowie \(W\) reelle Vektorräume.

\par
Wir nennen eine Abbildung
\begin{align*}
\varphi : \V_1\times\ldots\times \V_k\ \to W
\end{align*}
\par
\textbf{k (multi)linear}, falls alle zugehörigen partiellen Abbildungen \(\varphi_i\) für \(i\in\{1,\ldots,k\}\) mit
\begin{align*}
\varphi_i \colon V_i &\to W\\
x&\mapsto \varphi_i(x):= \varphi(z_1,\ldots, z_{i-1}, x, z_{i+1},\ldots,z_k)
\end{align*}
\par
\emph{linear} sind.

\par
Die Menge aller \(k\) linearen Abbildungen wird mit \(L^k(\V_1\times\ldots\times \V_k; W)\) bezeichnet.
Falls alle Vektorräume übereinstimmen, d.h., \(\V_i = \V\) für alle \(i=1,\ldots,k\) gilt, so schreibt man auch \(L^k(\V\times\ldots\times \V; W) =: L^k(\V; W)\).
\end{definition}
\begin{remark}{}{vektoranalysis/multilinear:remark-7}



\par
Ausgeschrieben bedeutet die Bedingung in der obigen Definition, dass für beliebige Vektoren \(x,y\in \V_i\) und Skalare \(\lambda \in \R\) gilt
\begin{align*}
\varphi(z_1,\ldots,z_{i-1},\lambda \cdot x, z_{i+1},\ldots,z_k) = \lambda \cdot \varphi(z_1,\ldots,z_{i-1}, x, z_{i+1}, \ldots,z_k)
\end{align*}
\par
und
\begin{align*}
\varphi(z_1,\ldots,z_{i-1},x+y,z_{i+1},\ldots,z_k) = \varphi(z_1,\ldots,x,\ldots,z_k) + \varphi(z_1,\ldots,y,\ldots,z_k).
\end{align*}
\par
für jedes Argument \(i = 1,\ldots,k\) der Abbildung \(\varphi \colon V_1 \times \ldots \times \V_k \rightarrow W\).
\end{remark}

\par
Viele multilineare Abbildungen kennen wir bereits aus der Linearen Algebra ohne sie bisher so bezeichnet zu haben.
Im folgenden Beispiel wiederholen wir einige bekannte Beispiele unter dem Aspekt der Multilinearität.
\begin{example}{}{vektoranalysis/multilinear:ex:multi}



\par
Wir betrachten im Folgenden Beispiele für \(k\) lineare Abbildungen mit verschiedenen \(k\in\N\).

\par
\textbf{\(k=1\)}: In diesem einfachen Fall sind alle Linearformen \(1\) linear.
Daher ist der Raum der \(1\) Linearformen gerade der algebraische Dualraum aus \cref{vektoranalysis/multilinear:def:algebraischerDualraum}  d.h. es gilt \(L^1(\V; \R) = \V^\ast\).

\par

\textbf{\(k=2\)}: Es sei \(\V=\R^n\) der Euklidische Vektorraum mit kanonischem innerem Produkt \(\langle\cdot,\cdot\rangle\).
Für \(A\in\R^{n,n}\) ist
\begin{align*}
\varphi:\V\times \V &\to\R\\ 
(x,y) &\mapsto \varphi(x, y) :=\langle x,A y \rangle
\end{align*}
\par
eine \textbf{Bilinearform} bzw. eine \(2\) Linearform nach \cref{vektoranalysis/multilinear:def:multilinear} 
Sie heißt \emph{symmetrisch}, falls
\begin{align*}
\varphi(x, y) = \varphi(y, x), \quad \forall x, y\in \V
\end{align*}
\par
und \emph{antisymmetrisch} falls
\begin{align*}
\varphi(x, y) = -\varphi(y, x), \quad \forall x, y\in \V.
\end{align*}
\par

\textbf{\(k=n\)}: Es sei \(n\in \N\) und \(\V=\R^n\) der Euklidische Vektorraum.
Die \(n\) lineare Abbildung
\begin{align*}
\varphi :\V \times \ldots \times \V &\to\R\\ 
(z_1, \ldots, z_n) &\mapsto \varphi(z_1,\ldots,z_n) := \det([z_1,\ldots,z_n])
\end{align*}
\par
heißt \textbf{Determinantenform}.
Wir beachten, dass hierbei jedes \(z_i \in \R^n\) für \(i=1,\ldots,n\) ein Vektor ist und es sich bei \([z_1,\ldots,z_n] \in \R^{n\times n}\) um eine Matrix handelt.
Die Determinantenform gibt das orientierte Volumen des von den Vektoren \(z_1,\ldots,z_n\) aufgespannten Parallelotops an.
\end{example}


\subsection{Der Vektorraum der Multilinearformen}
\label{\detokenize{vektoranalysis/multilinear:der-vektorraum-der-multilinearformen}}
\par
Die Definition einer \(k\) linearen Abbildung ermöglicht es uns sehr direkt eine Vektorraumstruktur zu definieren.
\begin{lemma}{}{vektoranalysis/multilinear:lemma-9}



\par
Es seien \(\V_1,\ldots,\V_k\) sowie \(W\) reelle Vektorräume, dann ist die Menge \(L^k(\V_1\times\ldots\V_k,W)\) ein Vektorraum über \(\R\) bezüglich der Addition
\begin{align*}
(\varphi_1+\varphi_2)(z_1,\ldots,z_k) := \varphi_1(z_1,\ldots,z_k) +
\varphi_2(z_1,\ldots,z_k),\quad \varphi_1,\varphi_2\in L^k(\V,\R)
\end{align*}
\par
und der Skalarmultiplikation
\begin{align*}
(\lambda\varphi)(z_1,\ldots,z_k) := \lambda\big(\varphi(z_1,\ldots,z_k)\big),\quad\varphi\in L^k(\V,\R), \lambda\in\R.
\end{align*}\end{lemma}

\begin{proof}
 Siehe Übung.
\end{proof}

\par
Als wichtigen Spezialfall erhalten wir für \(k=1\) den Dualraum \(L^1(\V,\R)\). Für diesen Vektorraum können wir eine spezielle Basis charakterisieren, die sogenannte \textbf{duale Basis}.
\begin{lemma}{(Duale Basis)}{vektoranalysis/multilinear:lemma-10}



\par
Es sei \(\V\) ein \(n\) dimensionaler \(\R\) Vektorraum mit einer Basis \(B = (b_1,\ldots,b_n)\), die Abbildungen
\(\eta_i:\V\rightarrow\R\) für \(i=1,\ldots,n\),
\begin{align*}
\eta_i(z) = \eta_i\left(\sum_{i=1}^n \alpha_i b_i\right) := \alpha_i
\end{align*}
\par
bilden einen Basis von \(\V^\ast\), die sogenannte \textbf{duale Basis} zu \(B\).
\end{lemma}
\begin{example}{(Duale Basis)}{vektoranalysis/multilinear:example-11}



\par
Rn mit Standardbasis ist es Multiplikation von rechts mit Transponierter Standardbasis.
\end{example}
\begin{remark}{}{vektoranalysis/multilinear:remark-12}



\par
Insbesondere zeigt diese Aussage, dass \(\dim(\V) = \dim(\V^\ast)\).
\end{remark}

\begin{proof}
 Wir zeigen zunächst, dass \(\eta_i\in\V^\ast\). Dazu sei \(x,y\in\V\), dann existieren skalare
\(\alpha_i^x,\alpha_i^y\in\R\) für \(i=1,\ldots,n\), s.d.,
\begin{align*}
x &= \sum_{i=1}^n \alpha_i^x b_i\\
y &= \sum_{i=1}^n \alpha_i^y b_i.
\end{align*}
\par
Somit haben wir
\begin{align*}
\eta_i(x+y) &= \eta_i\left(\sum_{i=1}^n \alpha_i^x b_i + \alpha_i^y b_i\right) 
\\&=
\eta_i\left(\sum_{i=1}^n (\alpha_i^x + \alpha_i^y) b_i\right) 
\\&=
\alpha_i^x + \alpha_i^y
\\&=
\eta_i\left(\sum_{i=1}^n \alpha_i^x b_i\right)  + \eta_i\left(\sum_{i=1}^n \alpha_i^y b_i\right)
\\&=
\eta_i(x) + \eta_i(y).
\end{align*}
\par
Weiterhin gilt für \(\lambda\in\R\)
\begin{align*}
\eta_i(\lambda x) &= \eta_i\left(\lambda \sum_{i=1}^n \alpha_i^x b_i\right) 
\\&=
\eta_i\left(\sum_{i=1}^n (\lambda \alpha_i^x) b_i\right) 
\\&=
\lambda \alpha_i^x
\\&=
\lambda \eta_i(x)
\end{align*}
\par
und damit ist \(\eta_i\) linear.
Sei nun \(\phi\in\V^\ast\), dann gilt
\begin{align*}
\phi(x) = \phi\left(\sum_{i=1}^n \alpha_i^x b_i\right) = \sum_{i=1}^n \alpha_i^x \phi(b_i) = 
\sum_{i=1}^n \eta_i(x) \phi(b_i),
\end{align*}
\par
insbesondere gilt also \(\phi = \sum_{i=1}^n \eta_i \phi(b_i\)).

\par
Somit bilden die Abbildungen \(\eta_i\) ein Erzeugenden System, da jedes \(\phi\) als Linearkombination dargestellt werden kann.
Weiterhin seien \(a_i\in\R\) gegeben, s.d., \(0 = \sum_{i=1}^n a_i \eta_i\), damit folgt für jedes \(j=1,\ldots,n\),
\begin{align*}
0 = \left(\sum_{i=1}^n a_i \eta_i\right)(b_j) = \sum_{i=1}^n a_i \eta_i(b_j) = a_j
\end{align*}
\par
und damit ist die Aussage bewiesen.
\end{proof}
\begin{remark}{}{vektoranalysis/multilinear:remark-13}



\par
Die Aussage lässt sich auf den Fall eines unendlich dimensionalen Vektorraums übertragen. Hierfür erinnern wir daran, dass für einen Vektorraum \(V\) stets eine Basis \(B^V = \{b_i^v:i\in I\}\subset V\) existiert wobei \(I\) eine (nicht notwendigerweise endliche) Indexmenge ist. Insbesondere bemerken wir, dass wir hier von einer \textbf{Hamelbasis} sprechen, d.h., für jedes Element \(v\in V\) gibt es eindeutig bestimmte Koeffizienten \(\alpha_i, i\in I\) s.d.
\begin{align*}
v = \sum_{i\in I} \alpha_i b_i.
\end{align*}
\par
Der wichtige Punkt ist aber, dass nur \textbf{endlich viele} \(\alpha_i\) ungleich null sind und die Summation somit keine eigentlich unendliche Reihe beschreibt sondern nur eine endliche Summe. Diese Konzept ist insbesondere verschieden vom Begriff der \href{https://de.wikipedia.org/wiki/Schauderbasis}{Schauderbasis}
\end{remark}

\begin{emphBox}{Georg Hamel}{}

\par
\href{https://de.wikipedia.org/wiki/Georg\_Hamel}{Georg Karl Wilhelm Hamel} (Geboren 12. September 1877 in Düren; Gestorben 4. Oktober 1954 in Landshut) war ein deutscher Mathematiker.
\end{emphBox}

\begin{emphBox}{Juliusz Schauder}{}

\par
\href{https://de.wikipedia.org/wiki/Juliusz\_Schauder}{Juliusz Paweł Schauder} (Geboren 21. September 1899 in Lemberg; Gestorben September 1943) war ein polnischer Mathematiker.
\end{emphBox}

\par
Wir halten weiterhin fest, dass sich der doppelt duale Raum im endlich dimensionalen Fall leicht charakterisieren lässt.
\begin{lemma}{}{vektoranalysis/multilinear:lem:doubledual}



\par
Es sei \(\V\) ein \(n\) dimensionaler reeller Vektorraum, dann gilt, dass die Abbildung
\begin{align*}
\Psi&:\V\rightarrow \V^{\ast\ast}\\
\Psi(x)&:= (\varphi\mapsto \varphi(x))
\end{align*}
\par
ein Isomorphismus ist.
\end{lemma}


\section{Tensoren und Tensorprodukte}
\label{\detokenize{vektoranalysis/tensor:tensoren-und-tensorprodukte}}\label{\detokenize{vektoranalysis/tensor::doc}}
\par
In diesem Kapitel widmen wir uns einem wichtigen aber komplizierten Thema der Vektoranalysis, nämlich Tensoren und Tensorprodukten.
Der Begriff hat sehr viele verschiedene Anschauungsmöglichkeiten (siehe \href{https://de.wikipedia.org/wiki/Tensorprodukt}{Wikipedia}) weshalb es nicht leicht ist eine Einführung zu geben die gleichzeitig allgemein, aber auch verständlich ist. Da Tensoren aber eine wichtige Rolle in der Physik spielen werden wir uns hier damit beschäftigen.


\subsection{Motivation}
\label{\detokenize{vektoranalysis/tensor:motivation}}
\par
Wir betrachten zwei Beispiele aus der Physik, welche auf Tensoren zurückgreifen.
\begin{remark}{}{vektoranalysis/tensor:remark-0}



\par
Der Begriff Tensor wurde von Hamilton in der Mitte des 19. Jahrhunderts eingeführt. Er leitete die Bezeichnung vom latinischen \emph{tendere} (spannen) ab, da die ursprüngliche Anwendung derartiger Objekte in der Elastizitätstheorie Anwendung fand.
\end{remark}


\subsubsection{Der Cauchy Spannungstensor}
\label{\detokenize{vektoranalysis/tensor:der-cauchy-spannungstensor}}
\begin{emphBox}{Augustin Cauchy}{}

\par
\href{https://de.wikipedia.org/wiki/Augustin-Louis\_Cauchy}{Augustin Louis Cauchy} (Geboren 21. August 1789 in Paris; Gestorben 23. Mai 1857 in Sceaux) war ein französischer Mathematiker.
\end{emphBox}

\par
Mechanische Spannung beschreibt die innere Beanspruchung und Kräfte in einem Volumen \(V\subset\R^3\) die aufgrund einer äußeren Belastungen auftreten. Die grundlegende Idee ist das \textbf{Euler Cauchy Stress Prinzip}, welches beschreibt, dass auf jede Schnittfläche \(A\subset\R^2\) welche ein Volumen in zwei Teile trennt, von diesen zwei Komponenten eine Spannung auf \(A\) ausgewirkt wird, welche durch den \textbf{Spannungsvektor} \(\mathbf{T}^n\) beschrieben wird. Der Spannungsvektor ist hierbei von der Dimension “Kraft pro Fläche”.

\begin{figure}[htbp]
\centering


\noindent\includegraphics[height=250\pxdimen]{{stress_vector}.png}
\caption{Visualisierung für Normal  und Scherspannung an einer Schnittfläche. Quelle: \href{https://en.wikipedia.org/wiki/Cauchy\_stress\_tensor}{Wikipedia; Cauchy Stress Tensor}.}\label{\detokenize{vektoranalysis/tensor:fig-stress}}\end{figure}

\par
Wie in \hyperref[\detokenize{vektoranalysis/tensor:fig-stress}]{Fig.\@ \ref{\detokenize{vektoranalysis/tensor:fig-stress}}} visualisiert teilt sich die Spannung in zwei Komponenten auf:

\par
\textbf{Normalspannung:}

\par
Dieser Teil des Spannungsvektor zeigt in Richtung der normalen \(\mathbf{n}\) welche orthogonal auf der Schnittfläche stehen.

\par
\textbf{Scherspannung:}

\par
Dieser Teil des Spannungstensors ist parallel zur Schnittfläche.

\par
Man erkennt nun, dass die Spannung in \(V\) nicht durch einen einzigen Vektor ausgedrückt werden kann. Einerseits hängt sie vom betrachteten Punkt \(P\in V\) ab und zudem von der Orientierung der Schnittfläche. Allerdings hat Cauchy gezeigt, dass ein Tensorfeld \(\mathbf{\sigma}(x)\) existiert, s.d.,
\begin{align*}
T^{\mathbf{n}}(x) = \mathbf{n}\cdot \mathbf{\sigma}(x),
\end{align*}
\par
d.h. in jedem Punkt \(x\in V\) ist der Stressvektor linear im Normalenvektor \(\mathbf{n}\).

\begin{figure}[htbp]
\centering


\noindent\includegraphics[height=250\pxdimen]{{stress_tensor_comp}.png}
\caption{Quelle: \href{https://de.wikipedia.org/wiki/Spannungstensor}{Wikipedia; Spannungstensor}.}\label{\detokenize{vektoranalysis/tensor:fig-stress-comp}}\end{figure}

\par
Hierfür betrachtet man einen freigeschnittenen Würfel wie in \hyperref[\detokenize{vektoranalysis/tensor:fig-stress-comp}]{Fig.\@ \ref{\detokenize{vektoranalysis/tensor:fig-stress-comp}}} und definiert für die drei verschiedenen Flächen (orthogonal zu den Einheitsvektoren) den Stresstensor
\begin{align*}
\mathbf{T}^{e_i}:= \sum_{j=1}^3 \sigma_{ij} e_j.
\end{align*}
\par
So haben wir z.B. für \(\mathbf{T}^{e_1}\) die Normalspannung gegeben durch \(\sigma_{11} e_1\) und die zwei Scherspannungskomponenten \(\sigma_{12} e_2, \sigma_{13} e_3\). Insgesamt erhält man neun Komponenten \(\sigma_{ij}\) welche über die Definition
\begin{align*}
\mathbf{\sigma} := \sum_{i=1}^3 e_i \otimes \mathbf{T}^{e_i} = \sum_{i=1}^3\sum_{j=1}^3 \sigma_{ij} (e_i\otimes e_j)
\end{align*}
\par
den Cauchy Stresstensor \(\mathbf{\sigma}\) ergebene. Hierbei bezeichnet \(\otimes\) das \textbf{dyadische Produkt} zweier Vektoren. Für \(x\in\R^n,y\in\R^m\) definieren wir
\begin{align*}
x \otimes y := 
\begin{pmatrix}
x_1y_1 &\ldots &x_1 y_m\\
\vdots &\ddots & \vdots\\
x_n y_1&\ldots& x_n y_m
\end{pmatrix}.
\end{align*}
\par
Wir werden später sehen, dass man die Idee \(\sigma\) über das dyadische Produkt zu definieren abstrahieren kann, was auf den allgemeinen Tensorbegriff führt.


\subsubsection{Quantenverschränkung}
\label{\detokenize{vektoranalysis/tensor:quantenverschrankung}}

\subsection{Das Tensorprodukt}
\label{\detokenize{vektoranalysis/tensor:das-tensorprodukt}}
\par
Wir wollen nun das Tensorprodukt von Vektorräumen abstrakt einführen und es später konkret realisieren.
\begin{definition}{(Tensorprodukt)}{vektoranalysis/tensor:definition-1}



\par
Es seien \(V,W\) zwei reelle Vektorräume. Ein reeler Vektorraum \(X\) heißt \textbf{Tensorproduktraum} falls eine bilineare Abbildung \(\otimes:V\times W\rightarrow X\) existiert, s.d., die folgende \textbf{universelle Eigenschaft} gilt:

\par
Für jede Bilinearform \(\phi\in L^2(V\times W, Y)\) in einen beliebigen reellen Vektorraum \(Y\), existiert eine eindeutige lineare Abbildung
\(p \in L^1(X, Y)\), s.d. gilt
\begin{align*}
\phi(v,w) = p((v\otimes w)) = p(\otimes(v,w))\quad\forall (v,w)\in V\times W.
\end{align*}
\par
In diesem Fall schreibt man auch \(X = V\otimes X\), \(\otimes\) heißt Tensorprodukt und zusätzlich ist die Schreibweise \(\otimes(v,w)=:v\otimes w\) üblich.
\end{definition}

\par
\textbf{Was bedeutet das?}

\par
Diese Definition erscheint auf den ersten Blick abstrakt und unverständlich. Was ist jetzt also ein Tensorprodukt?

\par
\textbf{Das Tensorprodukt ist universell:}

\par
Wir haben benutzten in der Definition oben das kartesische Produkt \(\times\) welches eindeutig definiert ist. Im Gegensatz dazu gibt es nicht \emph{ein} Tensorprodukt \(\otimes\) oder \emph{einen} Tensorproduktraum \(V\otimes W\). Wir haben die Freiheit \(\otimes\) zu wählen und wann immer die universelle Eigenschaft erfüllt ist, heißt dann \(V\otimes W\) Tensorproduktraum. Derartige Konzepte nennt man in der Algebra \emph{universell}.

\par
\textbf{Was bedeutet die universelle Eigenschaft?}

\par
Wie wir weiter unten noch genauer beschreiben werden, stellt die universelle Eigenschaft eine wichtige Beziehung zwischen dem Raum der bilinearen Abbildungen auf \(V\times W\) und dem Dualraum von \(V\otimes W\) her. Sofern wir das Tensorprodukt gegeben haben erhalten wir alle Bilinearformen schon über einfache Linearformen auf \(V\otimes W\).


\subsection{Existenz und Konstruktion}
\label{\detokenize{vektoranalysis/tensor:existenz-und-konstruktion}}
\par
Wir können ein Tensorprodukt konkret konstruieren indem wir uns auf die Basis der Vektorräume \(V\) und \(W\) zurückziehen. Diese Tatsache formulieren wir in der folgenden Aussage.
\begin{theorem}{}{vektoranalysis/tensor:theorem-2}



\par
Für zwei reelle Vektorräume \(V, W\) existiert stets mindestens ein Tensorprodukt \(\otimes\in L^2(V\times W, V\otimes W)\).
\end{theorem}

\begin{proof}
 Der folgende Beweis ist ein sogenannter konstruktiver Beweis, d.h., wir zeigen die Existenz eines Objekts indem wir es explizit angeben. Es gibt auch nicht konstruktive Existenzbeweise.

\par
Es sei \(B^V = \{b_i^V: i\in I^V\}\) eine Basis von \(V\) und analog \(B^W = \{b_i^W: i\in I^W\}\)  eine Basis von \(W\) für Indexmengen \(I^V, I^W\). Wir betrachten das kartesische Produkt
\begin{align*}
J := I^V \times I^W = \{(i,j): i\in I^V, j\in I^W\}.
\end{align*}
\par
Es sei nun \(X\) ein Vektorraum dessen Basis sich durch \(J\) indizieren lässt, d.h., es existiert eine Menge
\begin{align*}
B^X = \{b_{ij}^X: (i,j)\in J\}
\end{align*}
\par
s.d. \(B^X\) eine Basis von \(X\) ist. Ein solcher Vektorraum existiert, da z.B. das kartesische Produkt \(V\times W\) diese Eigenschaft erfüllt.

\par
Wir definieren nun eine bilineare Abbildung \(\otimes: V\times W\to X\) über
\begin{align*}
b_i^V \otimes b_j^W := b_{ij}^X\quad\forall (i,j)\in J.
\end{align*}
\par
Beachte, \(\otimes\) ist durch die Definition auf \(J\) eindeutig festgelegt, da für beliebige \((v,w)\in V\times W\) endlich viele Faktoren
\(\alpha_{i_1},\ldots,\alpha_{i_m}\) und \(\beta_{j_1},\ldots, \beta_{j_n}\) existieren s.d.
\begin{align*}
\otimes(v,w) 
&= 
\otimes\big(\sum_{k=1}^n \alpha_{i_k} b_{i_k}^V, \sum_{l=1}^m \beta_{j_l} b_{j_l}^W\big) 
\\&= 
\sum_{k=1}^n \sum_{l=1}^m \otimes\left(b_{i_k}^V, b_{j_l}^W\right)
\\&=
\sum_{k=1}^n \sum_{l=1}^m b_{i_kj_l}^X.
\end{align*}
\par
Wir müssen nun die universelle Eigenschaft zeigen, sei dazu \(\phi\in L^2(V\times W, Y)\) eine Bilinearform auf einen reellen Vektorraum \(Y\), dann können wir eine Linearform auf \(p:X\to Y\) definieren durch (analog reicht es die Definition auf den Basiselementen anzugeben)
\begin{align*}
p(b_{ij}^X) := \phi(b_i^V, b_j^W).
\end{align*}
\par
Dann gilt nämlich, unter Ausnutzung der Linearität von \(p\) und obiger Rechnung, dass
\begin{align*}
p(\otimes(v,w)) 
&=
\sum_{k=1}^n \sum_{l=1}^m p(b_{i_kj_l}^X)
\\&=
\sum_{k=1}^n \sum_{l=1}^m \phi\left(b_{i_k}^V, b_{j_l}^W\right)
\\&
\phi\big(\sum_{k=1}^n  b_{i_k}^V,\sum_{l=1}^m b_{j_l}^W\big)
\\&
\phi(v,w)
\end{align*}
\par
und somit gilt die universelle Eigenschaft. Insbesondere, da \(p\) durch die obige Definition eindeutig festgelegt ist.
\end{proof}

\par
Als Korollar erhalten wir somit, dass eine Basis des Tensorproduktraums durch das kartesische Produkt der ursprünglichen Basen konstruiert werden kann. Hieran sieht man qualitativ den Unterschied zwischen \(V\otimes W\) und \(V\otimes W\).
\label{vektoranalysis/tensor:corollary-3}
\begin{emphBox}{}{}{Corollary 3.1}



\par
Für zwei reelle Vektorräume \(V,W\) mit Basen \(B^V = \{b_i^V: i\in I^V\}, B^W = \{b_i^W: i\in I^W\}\) und ein Tensorprodukt \(\otimes:V\times W\to V\otimes W\) ist
\begin{align*}
\{b_i^V\otimes b_j^W: i\in I^V, j\in I^W\}
\end{align*}
\par
eine Basis von \(V\otimes W\).
\end{emphBox}

\par
Wir wissen nun, dass mindestens ein Tensorprodukt existiert, es stellt sich also die Frage inwiefern sich verschiedene derartige Abbildungen auf den gleichen Vektorräumen \(V,W\) unterschieden. Seien dazu \(\otimes_1, \otimes_2\) je zwei Tensorprodukte auf \(V\times W\). Wegen der universellen Eigenschaft gibt es lineare Abbildungen \(p_1: V\otimes_1 W\to W\otimes_2 V\) und \(p_2: V\otimes_2 W\to W\otimes_1 V\), s.d.,
\begin{align*}
\otimes_2 &= p_1 \circ \otimes_1\\
\otimes_1 &= p_2 \circ \otimes_2.
\end{align*}
\par
und somit
\begin{align*}
\otimes_2 &= p_1\circ p_2 \circ \otimes_2\\
\otimes_1 &= p_2\circ p_1 \circ \otimes_1.
\end{align*}
\par
Da wir aber die Basis von \(V\otimes_2 W\) über Elemente \(\otimes_2(b_i^V, b_j^W)\) charakterisieren können, und aus der ersten Gleichung folgt, dass
\begin{align*}
p_1\circ p_2(\otimes(b_i^V,b_j^W)) = \otimes(b_i^V, b_j^W)
\end{align*}
\par
wissen wir dass \(p_1\circ p_2 = \mathrm{Id}\). Das folgt da \(p_1\circ p_2\) als lineare Abbildung schon ganz auf den Basiselementen festgelegt ist.
Analog folgt \(p_2\circ p_1 = \mathrm{Id}\) und somit sind \(p_1, p_2\) isomorph zueinander. D.h. wir haben insgesamt gezeigt, dass verschiedene Tensorprodukte stets isomorph zueinander sind.
\begin{lemma}{}{vektoranalysis/tensor:lemma-4}



\par
Es seien \(V,W\) zwei reelle Vektorräume und \(\otimes_1,\otimes_2\) zwei Tensorprodukte. Dann existiert genau ein Isomorphismus \(p:V\otimes_1 W\to V\otimes_2 W\), s.d.
\begin{align*}
\otimes_2 = p\circ \otimes_1.
\end{align*}\end{lemma}


\subsection{Tensoren als Linearformen}
\label{\detokenize{vektoranalysis/tensor:tensoren-als-linearformen}}
\par
Als Einleitung in das Thema wollen wir Tensoren zunächst als Linearformen auf \(\V_1\times\ldots\times\V_k\)
betrachten wobei für \(i=1,\ldots,k\) \(\V_i\) reelle endlich dimensionale Vektorräume sind.
Man schreibt in diesem Fall auch
\begin{align*}
\V_1\otimes\ldots\otimes\V_k = L(\V_1\times\ldots\V_k,\R)
\end{align*}
\par
wobei \(\otimes\) das Tensorprodukt bezeichnet.

\par
Der wichtige Spezialfall ist hier allerdings nun nicht \(\V^k\) sondern ein kartesisches Produkt der Form
\begin{align*}
(V^\ast)^r\times V^s.
\end{align*}\begin{definition}{}{vektoranalysis/tensor:definition-5}



\par
Es sei \(\V\) ein reeller endlich dimensionaler Vektorraum, dann nennt man
\begin{align*}
T^r_s(V) := L((V^\ast)^r\times V^s, \R)
\end{align*}
\par
Menge der \(r\) fach \textbf{kontravarianten} und \(s\) fach \textbf{kovarianten} Tensoren, oder alternativ Tensoren der Stufe \((r,s)\).
\end{definition}

\par
Wir wollen diese abstrakte Definition nun mit einfachen Beispielen veranschaulichen zunächst für \(r+s=1\).
\begin{example}{}{vektoranalysis/tensor:example-6}



\par
Tensoren der Stufe \((1,0)\) können mit Elementen des Vektorraums selbst identifiziert werden, denn
\begin{align*}
T^1_0(V) = L((V^\ast), \R) = \V^{\ast\ast}\cong \V
\end{align*}
\par
mit der Identifikation aus \cref{vektoranalysis/multilinear:lem:doubledual}  Weiterhin sind Tensoren der Stufe \((0,1)\) Elemente des
Dualraums, also einfach Linearformen auf \(\V\), sogenannte \emph{Kovektoren}.
\end{example}

\par
Als weiteren Spezialfall erhalten wir Multilinearformen.
\begin{example}{}{vektoranalysis/tensor:example-7}



\par
Tensoren der Stufe \((0,k)\) sind \(k\) Linearformen, da \(T^0_k(V) = L^k(V)\).
\end{example}
\begin{example}{}{vektoranalysis/tensor:example-8}



\par
Aus einer linearen Abbildung \(A:\V\to\V\) erhält man direkt einen Tensor der Stufe \((1,1)\) über die Abbildung
\begin{align*}
\varphi, v \mapsto \varphi(Av).
\end{align*}\end{example}


\section{Differentialformen}
\label{\detokenize{vektoranalysis/diffformen:differentialformen}}\label{\detokenize{vektoranalysis/diffformen::doc}}
\par
In diesem Kapitel werden wir nun \href{https://de.wikipedia.org/wiki/Differentialform}{Differentialformen} einführen. Die entscheidende Neuerung im Vergleich zum vorhergehenden Kapitel, ist
dass wir zusätzlich zur Vektorraumstruktur nun ein Konzept von Räumlichkeit einführen, speziell betrachten wir eine offene Menge \(U\subset\R^n\). Ein weiterer wichtiger Aspekt, ist dass wir im Folgenden mit glatten Funktion arbeiten wollen, d.h., mit dem Raum \(C^\infty(U,\R^n)\).

\par
Eine Differentialform \(\omega\) auf \(U\subseteq\R^n\) ist eine von Ort zu Ort variierende äußere Form, deren Variation wir als glatt voraussetzen.

\par
Wir schreiben eine allgemeine \emph{\(k\)–Form} \(\omega\) in der \emph{Grundform}
\begin{align*}
\omega = \sum_{1\leq i_1<\ldots<i_k\leq n}\omega_{i_1\ldots i_k}
dx_{i_1}\wedge\ldots\wedge dx_{i_k}\in\Omega^k(U),
\end{align*}
\par
wobei
\begin{itemize}
\item {} 
\par
die \(\omega_{i_1\ldots i_k}\in \Omega^0(U):=C^\infty(U,\R)\), also glatte reelle Funktionen auf \(U\) sind,

\item {} 
\par
und die \(dx_i\) den Koordinatenfunktionen \(x_i:\R^n\to\R\) zugeordnete \(1\)–Differentialformen sind (\(dx_i\in\Omega^1(\R^n)\)).

\item {} 
\par
Den Raum der \(k\)–Differentialformen schreiben wir ab jetzt zur Unterscheidung vom Raum der äußeren \(k\)–Formen mit dem Symbol \(\Omega\) statt \(\Lambda\).

\end{itemize}

\par
Die \(dx_i\) sind durch ihre Wirkung auf ein Vektorfeld \(v:U\to
\R^n\) definiert, und \(dx_i(v)( y) := v_i( y)\).
\(1\)–Differentialformen machen also aus Vektorfeldern Funktionen, und für \(k\) Vektorfelder \(v^{(l)}:U\to\R^n\) ist für das \(\omega\) aus der Grundform
\begin{align*}
\omega\left(v^{(1)},\ldots,v^{(k)}\right) := \sum_{1\leq i_1<\ldots<i_k\leq n}
\omega_{i_1\ldots i_k}\cdot\det\begin{pmatrix} dx_{i_1}(v^{(1)})&\ldots& dx_{i_k}(v^{(1)})\\
\vdots&&\vdots\\
dx_{i_1}(v^{(k)})&\ldots& dx_{i_k}(v^{(k)}) \end{pmatrix}
\end{align*}
\par
definiert. Das Ergebnis ist also eine reelle Funktion auf \(U\).\textbackslash{}
Die Rechenregeln übertragen sich von den äußeren Formen auf die Differentialformen.

\par
Auf dem \(\R\)–Vektorraum
\begin{align*}
\Omega^*(U) := \bigoplus_{k=0}^n\Omega^k(U)
\end{align*}
\par
der Differentialformen betrachten wir jetzt
den \emph{Differentialoperator} \(d\), der durch
\begin{itemize}
\item {} 
\par
\(df := \sum_{i=1}^n\frac{\partial f}{\partial x_i}dx_i\) für Funktionen
\(f\in C^\infty(U,\R) = \Omega^0(U)\)

\item {} 
\par
und \(d\omega := \sum_{1\leq i_1<\ldots<i_k\leq n}d\omega_{i_1\ldots i_k}
\wedge dx_{i_1}\wedge\ldots\wedge dx_{i_k}\) für \(k\)–Formen \textbackslash{}linebreak
\(\omega = \sum_{1\leq i_1<\ldots<i_k\leq n}\omega_{i_1\ldots i_k}
dx_1\wedge\ldots\wedge dx_{i_k}\)

\end{itemize}

\par
definiert ist. \(d\) verwandelt eine \(k\)–Form also in eine \((k+1)\)–Form.
\begin{definition}{}{vektoranalysis/diffformen:aeussere Ableitung}



\par
Die lineare Abbildung \(d:\Omega^*(U)\to\Omega^*(U)\) heißt \href{https://de.wikipedia.org/wiki/\%C3\%84u\%C3\%9Fere\_Ableitung}{\textbf{äußere Ableitung}}.
\end{definition}
\begin{example}{(Äußere Ableitung)}{vektoranalysis/diffformen:ex:10.14}


\begin{enumerate}

\item {} 
\par
Für \(\omega\in\Omega^0(\R^3)\) ist \(d\omega = \frac{\partial\omega}{\partial x_1}dx_1+
\frac{\partial\omega}{\partial x_2}dx_2+\frac{\partial\omega}{\partial x_3}dx_3\).

\item {} 
\par
Für \(\omega = \omega_1dx_1+\omega_2dx_2+\omega_3dx_3\in\Omega^1(\R^3)\) ist

\end{enumerate}
\begin{align*}
d\omega &=& (d\omega_1)\wedge dx_1+(d\omega_2)\wedge dx_2+(d\omega_3)\wedge
dx_3\\
&=& \left(\frac{\partial\omega_2}{\partial x_1}-\frac{\partial\omega_1}{\partial x_2}\right)
dx_1\wedge dx_2+ \left(\frac{\partial\omega_3}{\partial x_2}-\frac{\partial\omega_2}{\partial x_3}\right)
dx_2\wedge dx_3\\
&& + \left(\frac{\partial\omega_1}{\partial x_3}-\frac{\partial\omega_3}{\partial x_1}\right)
dx_3\wedge dx_1
\end{align*}\begin{enumerate}

\item {} 
\par
Für \(\omega = \omega_{12}dx_1\wedge dx_2+\omega_{23}dx_2\wedge dx_3
+\omega_{31}dx_3\wedge dx_1 \in\Omega^2(\R^3)\) ist

\end{enumerate}
\begin{align*}
d\omega = \left(\frac{\partial\omega_{12}}{\partial x_3} + \frac{\partial\omega_{23}}{\partial x_1}
+ \frac{\partial\omega_{31}}{\partial x_2}\right)dx_1\wedge dx_2\wedge dx_3.
\end{align*}\begin{enumerate}

\item {} 
\par
Für \(\omega\in\Omega^3(\R^3)\) ist \(d\omega=0\).

\end{enumerate}
\end{example}
\begin{theorem}{}{vektoranalysis/diffformen:Antiderivation}



\par
\(d\) ist eine \href{https://de.wikipedia.org/wiki/Derivation\_(Mathematik)\#Antiderivationen}{\textbf{Antiderivation}}, d.h. für \(\alpha\in\Omega^k(U)\) und \(\beta\in\Omega^l(U)\) ist
\begin{align*}
d(\alpha\wedge\beta) = (d\alpha)\wedge\beta+(-1)^k\alpha\wedge d\beta.
\end{align*}\end{theorem}

\begin{proof}
 Wegen der Linearität von \(d\) genügt es, diese Gleichung für Monome
\begin{align*}
\alpha := f\underbrace{dx_{i_1}\wedge\ldots\wedge dx_{i_k}}_{\tilde
{\alpha}},\ \beta := g\underbrace{dx_{j_1}\wedge\ldots\wedge dx_{j_l}}_
{\tilde{\beta}},\ f,g\in C^\infty(U,\R)
\end{align*}
\par
zu beweisen.
Es gilt
\begin{align*}
d(\alpha\wedge\beta) &=& d(f\cdot g)\tilde{\alpha}\wedge
\tilde{\beta} = \big((df)g+f(dg)\big)\,\tilde{\alpha}\wedge\tilde{\beta}\\
&=& (df)\tilde{\alpha}\wedge g\tilde{\beta}+ (-1)^kf\tilde{\alpha}
\end{align*}\end{proof}
\begin{theorem}{}{vektoranalysis/diffformen:thm:dd}



\par
Auf \(\Omega^*(U)\) gilt
\end{theorem}

\begin{proof}
 1. Für \(f\in\Omega^0(U)\) ist
\begin{align*}
ddf &=& d\left(\sum_{i=1}^n\frac{\partial f}
{\partial x_i}dx_i\right) = \sum_{i=1}^n\sum_{l=1}^n\frac{\partial^2f}{\partial x_l\partial x_i}
dx_l\wedge dx_i\\
& =& \sum_{1\leq r< s\leq n}\left(\frac{\partial^2 f}{\partial x_r
\partial x_s} - \frac{\partial^2f}{\partial x_s\partial x_r}\right)dx_r\wedge dx_s = 0,
\end{align*}
\par
da wir wegen der Glattheit von \(f\) die partiellen Ableitungen vertauschen
können.
\begin{enumerate}

\item {} 
\par
Für \(\omega = \sum\omega_{i_1\ldots i_k}dx_{i_1}\wedge\ldots\wedge dx_{i_k}
\in\Omega^k(U)\) ist\textbackslash{}

\end{enumerate}
\begin{align*}
dd\omega = \sum(\underbrace{dd\omega_{i_1\ldots i_k}}_0)
\wedge dx_{i_1}\wedge\ldots\wedge dx_{i_k} = 0,
\end{align*}
\par
denn gemäß Satz \cref{vektoranalysis/diffformen:Antiderivation} wird die äußere Ableitung auf die
1 Formen \(d\omega_{i_1\ldots i_k}\) und \(dx_{i_l}\) angewandt, und nach Teil 1.
ist das Ergebnis Null.
\end{proof}
\begin{definition}{}{vektoranalysis/diffformen:geschlossen:exakt}



\par
Eine Differentialform \(\vv\in\Omega^*(U)\) heißt
\begin{itemize}
\item {} 
\par
\textbf{geschlossen}, wenn \(d\vv=0\), *\textbf{exakt}, wenn \(\vv=d\psi\) für ein \(\psi\in\Omega^*(U)\) gilt.

\end{itemize}

\par
Nach Satz \cref{vektoranalysis/diffformen:thm:dd} sind exakte Differentialformen geschlossen.\textbackslash{} Für \(k\)–Formen auf konvexen offenen Teimengen \(U\subseteq \R^n\) gilt für \(k\ge 1\)auch die Umkehrung (sog.
\href{https://de.wikipedia.org/wiki/Poincar\%c3\%a9-Lemma}{\textbf{Poincaré Lemma}} ),  siehe Kapitel \code{\upquote{sect:Poinca}}).
\end{definition}


