\chapter{Vektoranalysis}
\label{\detokenize{vektoranalysis/vektoranalysis:vektoranalysis}}\label{\detokenize{vektoranalysis/vektoranalysis::doc}}
\par
In diesem Kapitel führen wir wichtige Konzepte der \emph{Vektoranalysis} ein. Insbesondere schaffen wir die Grundlagen für eine spezielle Art der Mehrdimensionalen Integration, das Integrieren über sog. \emph{Untermannigfaltigkeiten} des \(\R^n\).
Um diese Integration durchzuführen, entwickeln wir das Kalkül der \emph{Differentialformen} auf Mannigfaltigkeiten.

\par
Dieses Kalkül lässt auch den \emph{geometrischen Gehalt} physikalischer
Theorien wie Elektrodynamik oder Allgemeine Relativitätstheorie klar
hervortreten. So lassen sich beispielsweise die sog. Maxwellschen Gleichungen der
Elektrodynamik in Differentialformenkalkül schreiben.

\par
Als zusätzliche Literatur und Referenz für diese Thematiken empfehlen wir das Buch von \emph{Agricola} und \emph{Friedrich}, \cite{AF13}.


\section{Multilinearformen}
\label{\detokenize{vektoranalysis/multilinear:multilinearformen}}\label{\detokenize{vektoranalysis/multilinear::doc}}
\par
In diesem Abschnitt wollen wir Multilinearformen kennenlernen. Für Vektorräume \(\V, W\) über einem Körper \(\K\) haben Sie bereits den Begriff der Linearform, also einer linearen Abbildung \(\varphi:\V\rightarrow W\) kennengelernt. Die Idee der Multilinearform ist anstatt einem, gleich \(k\) viele Vektorräume \(V_1,\ldots,V_k\) über \(\K\) zu betrachten und das Konzept der Lineratität auf Abbildung \(\varphi:\V_1\times\ldots\V_k\rightarrow W\) zu übertragen. Zur Vereinfachung werden wir im folgenden nur den Körper \(\K=\R\) betrachten, in den meisten Fällen lassen sich die Konzepte aber direkt auf allgemeine Körper übertragen.

\par
Wir beginnen zunächst mit einer Wiederolung und betrachten die schon bekannten Linearformen. Insbesondere soll der nächste Abschnitt als Wiederholung zum vorherigen Semester die verschiedenen Begriffe des Dualraums abgrenzen.


\subsection{Dualräume}
\label{\detokenize{vektoranalysis/multilinear:dualraume}}
\par
Für einen reellen Vektorraum \(\V\) wollen wir lineare Abbildung \(\varphi:V\to\R\) betrachten.
\label{vektoranalysis/multilinear:definition-0}
\begin{definition}{}{}



\par
Es sei \(\V\) ein \(\R\) Vektorraum, die Menge
\begin{align*}
\V^\ast := \{\varphi:\V\rightarrow\R: \varphi\text{ ist linear}\}
\end{align*}
\par
heißt \textbf{algebraischer Dualraum}.
\end{definition}

\par
Aus \cite{Ten21} ist bereits der Begriff des \emph{topologischen Dualraums} bekannt, welcher allerdings eine etwas restriktivere Definition hat. Sie fordert noch zusätzlich die Stetigkeit der linearen Abbildungen.

\begin{sphinxadmonition}{danger}{Danger:}
\par
Der algebraische Dualraum ist im allgemeinen nicht gleich dem topologischen. Der Hauptzweck dieses Abschnitts ist es diese Tatsache klar zu machen und die Unterschiede der beiden Definitionen zu verstehen.
\end{sphinxadmonition}
\label{vektoranalysis/multilinear:definition-1}
\begin{definition}{}{}



\par
Es sei \(\V\) ein normierter \(\R\) Vektorraum für einen Körper \(\R\), dann heißt die Menge
\begin{align*}
\V^\prime := \{\varphi:\V\rightarrow\R: \varphi\text{ ist linear und stetig}\}
\end{align*}
\par
\textbf{topologischer Dualraum}.
\end{definition}
\label{vektoranalysis/multilinear:remark-2}
\begin{emphBox}{}{}{Remark 3.1}



\par
Damit die obige Definition sinnvoll ist, ist es in der Tat nicht notwendig, dass \(X\) ein normierter Raum ist. Es reicht anzunehmen, dass \(\V\) ein topologischer Vektorraum ist.
\end{emphBox}

\par
In unserem Kontext spielt allerdings der Begriff des algebraischen Dualraums eine wichtige Rolle, welcher im Folgenden eingeführt wird. Man erkennt sofort, dass stets \(\V^\prime\subset \V^\ast\) gilt. Weiterhin stimmen die beiden Räume im endlich dimensionalen Fall überein.
\label{vektoranalysis/multilinear:lemma-3}
\begin{lemma}{}{}



\par
Für \(n\in\N\) sei \(\V\) ein \(n\) dimensionaler \(\R\) Vektorraum, dann gilt
\begin{align*}
V^\prime = V^\ast.
\end{align*}\end{lemma}
\label{vektoranalysis/multilinear:remark-4}
\begin{emphBox}{}{}{Remark 3.2}



\par
Die Norm auf \(\V\) in der obigen Aussage ist durch das Standardskalarprodukt induziert.
\end{emphBox}

\begin{proof}
 Siehe Übung.
\end{proof}


\subsection{k Multilinearformen}
\label{\detokenize{vektoranalysis/multilinear:k-multilinearformen}}
\par
Wir verallgeminern nun den Begriff der Linearität in der folgenden Definition.
\label{vektoranalysis/multilinear:def:multilinear}
\begin{definition}{}{}



\par
Für \(i=1,\ldots,k\) sei \(\V_i\), sowie \(W\) ein reeller Vektorraum. Eine Abbildung
\begin{align*}
\varphi:\V_1\times\ldots\times \V_k\ \to W
\end{align*}
\par
heißt k \textbf{(multi)linear}, wenn für beliebige \(z_i\in\V_i\) und jede Komponente \(i\in\{1,\ldots,k\}\) die Abbildung
\begin{align*}
V_i &\to W\\
x&\mapsto \varphi_i(x):= \varphi(z_1,\ldots, z_{i-1}, x, z_{i+1},\ldots,z_k)
\end{align*}
\par
linear ist. Die Menge aller \(k\) linearen Abbildungen wird mit \(L^k(\V_1\times\ldots\times \V_k, W)\) bezeichnet.
\end{definition}
\label{vektoranalysis/multilinear:remark-6}
\begin{emphBox}{}{}{Remark 3.3}



\par
Ausgeschrieben bedeutet die Bedingung in der obigen Definition, dass für alle \(z_i\in V_i\), \(\lambda\in\R\),
und insbesondere für jedes \(i\in\{1,\ldots,k\}\), \(x,y\in \V_i\)  gilt,
\begin{align*}
\varphi(z_1,\ldots,z_{i-1},\lambda x, z_{i+1},\ldots,z_k) = \lambda
\varphi(z_1,\ldots,z_{i-1}, x, z_{j+1}, \ldots,z_k)
\end{align*}
\par
und
\begin{align*}
&\varphi(z_1,\ldots,z_{i-1},x+y,z_{j+1},\ldots,z_k)\\
= 
&\varphi(z_1,\ldots,x,\ldots,z_k) + \varphi(z_1,\ldots,y,\ldots,z_k).
\end{align*}\end{emphBox}

\par
Falls alle Vektorräume übereinstimmen, d.h., \(\V_i = \V\) für alle \(i=1,\ldots,k\), so schreibt man auch \(L^k(\V\times\ldots\times \V,W) = L^k(\V,W)\).

\par
Viele multilineare Abbildungen sind schon aus der Linearen Algebra vertraut. Im folgenden Beispiel wiederholen wir einige bekannte Beispiele unter dem Aspekt der Multilinearität.
\label{vektoranalysis/multilinear:ex:multi}
\begin{example}{}{}



\par
Wir betrachten Beispiele für verschiedene \(k\in\N\).

\par
\textbf{\(k=1\)}:

\par
In diesem Fall haben wir bereits gesehen, dass \(L^1(\V,\R) = \V^\ast\).

\par
\textbf{\(k=2\)}:

\par
Es sei \(\V=\R^n\) mit kanonischem innerem Produkt \(\langle\cdot,\cdot\rangle\). Für \(A\in\R^{n,n}\) ist
\begin{align*}
\varphi:\V\times \V\to\R\\ 
\varphi(x, y) :=\langle x,A y \rangle
\end{align*}
\par
eine \textbf{Bilinearform}. Sie heißt \emph{symmetrisch},
falls
\begin{align*}
\varphi(x, y) = \varphi( y, x)\qquad (x, y\in \V)
\end{align*}
\par
und \emph{antisymmetrisch} falls
\begin{align*}
\varphi(x, y) = -\varphi( y, x)\qquad (x, y\in \V).
\end{align*}
\par
\textbf{\(k=n\)}:

\par
Es sei \(\V=\R^n\). Die \(n\) lineare Abbildung
\begin{align*}
\varphi(z_1,\ldots,z_n) := \det((z_1,\ldots,z_n))
\end{align*}
\par
heißt \textbf{Determinantenform}. Wir beachten, dass hierbei jedes \(z_i\in\R^n\) ein Vektor und \((z_1,\ldots,z_n)\) eine Matrix ist.
Die Form gibt das orientierte Volumen des von \(z_1,\ldots,z_n\) aufgespannten Parallelotops an.
\end{example}


\subsection{Der Vektorraum der Multilinearformen}
\label{\detokenize{vektoranalysis/multilinear:der-vektorraum-der-multilinearformen}}
\par
Die Definition einer \(k\) linearen Abbildung ermöglicht es uns sehr direkt eine Vektorraumstruktur zu definieren.
\label{vektoranalysis/multilinear:lemma-8}
\begin{lemma}{}{}



\par
Es seien \(\V_1,\ldots,\V_k\) sowie \(W\) reelle Vektorräume, dann ist die Menge \(L^k(\V_1\times\ldots\V_k,W)\) ein Vektorraum über \(\R\) bezüglich der Addition
\begin{align*}
(\varphi_1+\varphi_2)(z_1,\ldots,z_k) := \varphi_1(z_1,\ldots,z_k) +
\varphi_2(z_1,\ldots,z_k),\quad \varphi_1,\varphi_2\in L^k(\V,\R)
\end{align*}
\par
und der Skalarmultiplikation
\begin{align*}
(\lambda\varphi)(z_1,\ldots,z_k) := \lambda\big(\varphi(z_1,\ldots,z_k)\big),\quad\varphi\in L^k(\V,\R), \lambda\in\R.
\end{align*}\end{lemma}

\begin{proof}
 Siehe Übung.
\end{proof}

\par
Als wichtigen Spezialfall erhalten wir für \(k=1\) den Dualraum \(L^1(\V,\R)\). Für diesen Vektorraum können wir eine spezielle Basis charakterisieren, die sogenannte \textbf{duale Basis}.
\label{vektoranalysis/multilinear:lemma-9}
\begin{lemma}{}{}



\par
Es sei \(\V\) ein \(n\) dimensionaler \(\R\) Vektorraum mit einer Basis \(B = (b_1,\ldots,b_n)\), die Abbildungen
\(\eta_i:\V\rightarrow\R\) für \(i=1,\ldots,n\),
\begin{align*}
\eta_i(z) = \eta_i\left(\sum_{i=1}^n \alpha_i b_i\right) := \alpha_i
\end{align*}
\par
bilden einen Basis von \(\V^\ast\), die sogenannte \textbf{duale Basis} zu \(B\).
\end{lemma}
\label{vektoranalysis/multilinear:remark-10}
\begin{emphBox}{}{}{Remark 3.4}



\par
Insbesondere zeigt diese Aussage, dass \(\dim(\V) = \dim(\V^\ast)\).
\end{emphBox}

\begin{proof}
 Wir zeigen zunächst, dass \(\eta_i\in\V^\ast\). Dazu sei \(x,y\in\V\), dann existieren skalare
\(\alpha_i^x,\alpha_i^y\in\R\) für \(i=1,\ldots,n\), s.d.,
\begin{align*}
x &= \sum_{i=1}^n \alpha_i^x b_i\\
y &= \sum_{i=1}^n \alpha_i^y b_i.
\end{align*}
\par
Somit haben wir
\begin{align*}
\eta_i(x+y) &= \eta_i\left(\sum_{i=1}^n \alpha_i^x b_i + \alpha_i^y b_i\right) 
\\&=
\eta_i\left(\sum_{i=1}^n (\alpha_i^x + \alpha_i^y) b_i\right) 
\\&=
\alpha_i^x + \alpha_i^y
\\&=
\eta_i\left(\sum_{i=1}^n \alpha_i^x b_i\right)  + \eta_i\left(\sum_{i=1}^n \alpha_i^y b_i\right)
\\&=
\eta_i(x) + \eta_i(y).
\end{align*}
\par
Weiterhin gilt für \(\lambda\in\R\)
\begin{align*}
\eta_i(\lambda x) &= \eta_i\left(\lambda \sum_{i=1}^n \alpha_i^x b_i\right) 
\\&=
\eta_i\left(\sum_{i=1}^n (\lambda \alpha_i^x) b_i\right) 
\\&=
\lambda \alpha_i^x
\\&=
\lambda \eta_i(x)
\end{align*}
\par
und damit ist \(\eta_i\) linear.
Sei nun \(\phi\in\V^\ast\), dann gilt
\begin{align*}
\phi(x) = \phi\left(\sum_{i=1}^n \alpha_i^x b_i\right) = \sum_{i=1}^n \alpha_i^x \phi(b_i) = 
\sum_{i=1}^n \eta_i(x) \phi(b_i),
\end{align*}
\par
insbesondere gilt also \(\phi = \sum_{i=1}^n \eta_i \phi(b_i\)).

\par
Somit bilden die Abbildungen \(\eta_i\) ein Erzeugenden System, da jedes \(\phi\) als Linearkombination dargestellt werden kann.
Weiterhin seien \(a_i\in\R\) gegeben, s.d., \(0 = \sum_{i=1}^n a_i \eta_i\), damit folgt für jedes \(j=1,\ldots,n\),
\begin{align*}
0 = \left(\sum_{i=1}^n a_i \eta_i\right)(b_j) = \sum_{i=1}^n a_i \eta_i(b_j) = a_j
\end{align*}
\par
und damit ist die Aussage bewiesen.
\end{proof}
\label{vektoranalysis/multilinear:remark-11}
\begin{emphBox}{}{}{Remark 3.5}



\par
Die Aussage lässt sich auf den Fall eines unendlich dimensionalen Vektorraums übertragen. Hierfür erinnern wir daran, dass für einen Vektorraum \(V\) stets eine Basis \(B^V = \{b_i^v:i\in I\}\subset V\) existiert wobei \(I\) eine (nicht notwendigerweise endliche) Indexmenge ist. Insbesondere bemerken wir, dass wir hier von einer \textbf{Hamelbasis} sprechen, d.h., für jedes Element \(v\in V\) gibt es eindeutig bestimmte Koeffizienten \(\alpha_i, i\in I\) s.d.
\begin{align*}
v = \sum_{i\in I} \alpha_i b_i.
\end{align*}
\par
Der wichtige Punkt ist aber, dass nur \textbf{endlich viele} \(\alpha_i\) ungleich null sind und die Summation somit keine eigentlich unendliche Reihe beschreibt sondern nur eine endliche Summe. Diese Konzept ist insbesondere verschieden vom Begriff der \href{https://de.wikipedia.org/wiki/Schauderbasis}{Schauderbasis}
\end{emphBox}

\begin{emphBox}{Georg Hamel}{}

\par
\href{https://de.wikipedia.org/wiki/Georg\_Hamel}{Georg Karl Wilhelm Hamel} (Geboren 12. September 1877 in Düren; Gestorben 4. Oktober 1954 in Landshut) war ein deutscher Mathematiker.
\end{emphBox}

\begin{emphBox}{Juliusz Schauder}{}

\par
\href{https://de.wikipedia.org/wiki/Juliusz\_Schauder}{Juliusz Paweł Schauder} (Geboren 21. September 1899 in Lemberg; Gestorben September 1943) war ein polnischer Mathematiker.
\end{emphBox}

\par
Wir halten weiterhin fest, dass sich der doppelt duale Raum im endlich dimensionalen Fall leicht charakterisieren lässt.
\label{vektoranalysis/multilinear:lem:doubledual}
\begin{lemma}{}{}



\par
Es sei \(\V\) ein \(n\) dimensionaler reeller Vektorraum, dann gilt, dass die Abbildung
\begin{align*}
\Psi:\V\rightarrow \V^{\ast\ast}
\Psi(x) := (\varphi\mapsto \varphi(x))
\end{align*}
\par
ein Isomorphismus ist.
\end{lemma}


\subsection{Äußere Formen}
\label{\detokenize{vektoranalysis/multilinear:auszere-formen}}
\par
In \cref{vektoranalysis/multilinear:ex:multi} haben wir für \(k=2\) bereits den Begriff der Antisymmetrie kennengelernt. Dieser Fall lässt sich auf beliebige \(k\in\N\) verallgemeinern, was zur Definition der äußeren Form führt.
\label{vektoranalysis/multilinear:aeussere_Form}
\begin{definition}{}{}



\par
Es sei \(\V\) ein \(n\)–dimensionaler \(\R\) Vektorraum und \(k\in\N\). Dann heißt \(\varphi\in L^k(E,\R)\)
\textbf{äußere} \(k\)\textbf{ Form},wenn sie \textbf{antisymmetrisch} ist, d.h., für alle \(1\leq i<l\leq k\) und
\(z\in \V^k\) gilt
\begin{align*}
\varphi(z_1,\ldots,z_i,\ldots,z_l,\ldots,z_k) =-\varphi(z_1,\ldots,z_l,\ldots,z_i,\ldots,z_k).
\end{align*}
\par
Der Unterraum der äußeren \(k\) Formen wird mit \(\Lambda^k(\V)\subset L^k(\V,\R)\) bezeichnet.
\end{definition}
\label{vektoranalysis/multilinear:example-14}
\begin{example}{}{}



\par
\(k=1\):

\par
Hier fallen alle bisherigen Definitionen zusammen, d.h.,
\begin{align*}
\Lambda^1(\V) = L^1(\V,\R)= \V^\ast
\end{align*}
\par
\(k=2\):

\par
Für \(A\in\R^{2,2}\) definiert die Abbildung \((x,y)\mapsto\langle x,A y\rangle\) eine äußere \(2\) Form auf \(\R^n\) genau dann, wenn die Matrix \(A\) antisymmetrisch ist. D.h., falls \(A^T=-A\) gilt.

\par
\(k=n\):

\par
Die Determinantenform ist bis auf ihre Vielfachen die einzige äußere \(n\) Form auf dem \(\R^n\).
\end{example}

\par
Wir beweisen zwei kleine Hilfsaussagen zu äußeren Formen
\label{vektoranalysis/multilinear:lemma-15}
\begin{lemma}{}{}



\par
Es sein \(\V\) ein reeller Vektorraum und \(\varphi\in\Lambda^k(V)\).
\begin{itemize}
\item {} 
\par
Für jede Permutation \(\pi:\{1,\ldots,k\}\rightarrow\{1,\ldots,k\}\) gilt

\end{itemize}
\begin{align*}
\varphi(z_{\pi(1)},\ldots,z_{\pi(k)}) = \sign(\pi) \varphi(z_1,\ldots,z_k).
\end{align*}\begin{itemize}
\item {} 
\par
Sind \(z_1,\ldots, z_k\) linear abhängig, so gilt \(\varphi(z_1,\ldots,z_k) = 0\).

\end{itemize}
\end{lemma}

\begin{proof}
 Die erste Behauptung folgt direkt aus der Tatsache, dass sich jede Permutation als Verkettung endlich vieler Transpositionen schreiben lässt.
Für die zweite Behauptung sehen wir zunächst, dass
\begin{align*}
\varphi(z_1,\ldots,x,\ldots, x,\ldots,z_k) = -\varphi(z_1,\ldots,x,\ldots,x,\ldots,z_k)
\end{align*}
\par
und somit \(\varphi(z_1,\ldots,x,\ldots,x,\ldots,z_k)=0\). Sind die \(z_i\) nun linear abhängig, so existieren Skalare \(\alpha_i\), s.d.,
ein \(j\) existiert mit \(\alpha_j\neq 0\) und
\begin{align*}
\sum_{i=1}^n \alpha_i z_i = 0 \Leftrightarrow z_j = 
\frac{1}{\alpha_j} \sum_{i\neq j} \alpha_i z_i.
\end{align*}
\par
Somit folgt
\begin{align*}
\varphi(z_1,\ldots,z_j,\ldots,z_k) = 
\sum_{i\neq j} \alpha_i \varphi(z_1,\ldots,z_{j-1},z_i,z_{j+1},\ldots,z_k) = 0.
\end{align*}\end{proof}

\par
Wir werden nun eine Methode kennelernen die es uns erlaubt eine äußere \(k\) Form als sogenanntes \textbf{äußeres Produkt} von \(k\) vielen Linearformen zu erhalten.
\label{vektoranalysis/multilinear:definition-16}
\begin{definition}{}{}



\par
Für einen Vektorraum \(\V\) ist das \textbf{äußere Produkt} von \(\omega_1,\ldots,\omega_k\in\Lambda^1(\V)\)
durch
\begin{align*}
\omega_1\wedge\ldots\wedge\omega_k:V^k&\to\R\\
(z_1,\ldots,z_k)&\mapsto 
\det
\begin{pmatrix}
\omega_1(z_1)&\ldots&\omega_k(z_1)\\ 
\vdots&&\vdots\\
\omega_1(z_k)&\ldots&\omega_k(z_k)
\end{pmatrix}
\end{align*}
\par
definiert.
\end{definition}
\label{vektoranalysis/multilinear:lemma-17}
\begin{lemma}{}{}



\par
Für einen Vektorraum \(\V\) ist das äußere Produkt von \(\omega_1,\ldots,\omega_k\in\Lambda^1(\V)\) eine \(k\) Linearform.
\end{lemma}

\begin{proof}
 Siehe Übung.
\end{proof}

\par
Insbesondere gilt damit für die Dualbasis \(\eta_1,\ldots,\eta_n\) von \(\V^*\), dass
\begin{align*}
\eta_{i_1}\wedge\ldots\wedge\eta_{i_k}\in\Lambda^k(\V)
\end{align*}
\par
für beliebige Indexkombinationen \(i_1,\ldots,i_k \in \{1,\ldots,n \}\). Wegen der Eigenschaften der Determinante gilt
\begin{align*}
\eta_{i_1}\wedge\ldots\wedge\eta_{i_k} = \sign(\pi)\, \eta_{\pi(i_1)}\wedge\ldots\wedge\eta_{\pi(i_k)}\end{align*}
\par
wobei \(\pi:\{i_1,\ldots,i_k\}\rightarrow\{i_1,\ldots,i_k\}\) eine Permutation ist, s.d.,
\begin{align*}
\pi(i_1) <= \ldots <= \pi(i_j) <= \ldots <= \pi(i_k).
\end{align*}
\par
Desweiteren gilt auch
\begin{align*}
\eta_{i_1}\wedge\ldots\wedge\eta_{i_k} &\neq 0\\
&\Leftrightarrow \\
i_{j}\neq i_l&\text{ für } j\neq l.
\end{align*}
\par
Wir können nun jede \(k\) Form \(\omega\in\Lambda^k(E)\) eindeutig als Linearkombination
\begin{align*}
\omega = \sum_{1\leq i_1<\ldots<i_k\leq n}\omega_{i_1\ldots i_k}
\alpha_{i_1}\wedge\ldots\wedge\alpha_{i_k}
\end{align*}
\par
mit Koeffizienten
\begin{align*}
\omega_{i_1\ldots i_k} := \omega(e_{i_1},\ldots,e_{i_k})\in\R
\end{align*}
\par
darstellen. Da
die Indexmengen \(\{i_1,\ldots ,i_k\}\) die \(k\)–elementigen Teilmengen von\(\{1,\ldots,n\}\) durchlaufen, gilt
für \(\dim(E)=n\)
\begin{align*}
\dim\left(\Lambda^k(E)\right) = {n\choose k}.
\end{align*}
\par
Das  \emph{äußere Produkt}
Produkt der \(k\)–Form \(\omega\) mit einer \(l\)–Form
\begin{align*}
\psi := \sum_{1\leq j_1<\ldots<j_{l}\leq n}\psi_{j_1\ldots j_l}\,
\alpha_{j_1}\wedge\ldots\wedge\alpha_{j_l}
\end{align*}
\par
wird nun distributiv als \(\omega\wedge\psi\in\Lambda^{k+l}(E)\),
\begin{align*}
\omega\wedge\psi := \sum_{1\leq i_1<\ldots<i_k\leq n} \sum_{1\leq
j_1<\ldots<j_l\leq n} \omega_{i_1\ldots i_k} \psi_{j_1\ldots j_l}
\alpha_{i_1}\wedge\ldots\wedge\alpha_{i_k}\wedge\alpha_{j_1}\wedge
\ldots\wedge\alpha_{j_l}
\end{align*}
\par
definiert. All diejenigen Summanden, bei denen ein Indexpaar \(i_r=j_s\)vorkommt, sind gleich Null, denn \(\alpha_l\wedge\alpha_l = -\alpha_l\wedge\alpha_l=0\).
\begin{itemize}
\item {} 
\par
Das äußere Produkt ist \emph{assoziativ}, d.h. für beliebige äußere Formen auf \(E\) gilt

\end{itemize}
\begin{align*}
(\omega\wedge\psi)\wedge\rho = \omega\wedge(\psi\wedge\rho).
\end{align*}\begin{itemize}
\item {} 
\par
Weiter gilt für eine \(k\)–Form \(\omega\) und eine \(l\)–Form \(\psi\)

\end{itemize}
\begin{align*}
\omega\wedge\psi = (-1)^{k\cdot l}\psi\wedge\omega,
\end{align*}
\par
denn wir müssen \(k\!\cdot\! l\)–mal Eins–Formen kommutieren, um von der einen
zur anderen Gestalt zu gelangen.
\label{vektoranalysis/multilinear:symplektische Form auf dem $\R^{2n}$}
\begin{example}{}{})}


\begin{align*}
\omega := \sum_{i=1}^n\alpha_i\wedge\alpha_{i+n}\in\Lambda^2(\R^{2n}).
\end{align*}
\par
Für \(n=2\) ergibt sich
\begin{align*}
\omega = \alpha_1\wedge\alpha_3+\alpha_2\wedge\alpha_4,
\end{align*}
\par
also
\begin{align*}
\omega\wedge\omega &=& (\alpha_1\wedge\alpha_3+\alpha_2\wedge\alpha_4)
\wedge(\alpha_1\wedge\alpha_3+\alpha_2\wedge\alpha_4)\\
&=& \underbrace{\alpha_1\wedge\alpha_3\wedge\alpha_1\wedge\alpha_3}_0 +
\alpha_2\wedge\alpha_4\wedge\alpha_1\wedge\alpha_3\\
&& + \alpha_1\wedge\alpha_3\wedge\alpha_2\wedge\alpha_4 + \underbrace
{\alpha_2\wedge\alpha_4\wedge\alpha_2\wedge\alpha_4}_0\\
&=& (-1)^3\alpha_1\wedge\alpha_2\wedge\alpha_3\wedge\alpha_4 +
(-1)^1\alpha_1\wedge\alpha_2\wedge\alpha_3\wedge\alpha_4\\
&=& -2\alpha_1\wedge\alpha_2\wedge\alpha_3\wedge\alpha_4.
\end{align*}
\par
Die symplektische Form \(\omega\) hat eine Schlüsselrolle in der Klassischen Mechanik. Dort bezeichnet man die Koordinaten \(x_1,\ldots,
x_n\) als Impulskoordinaten, die Koordinaten \(x_{n+1},\ldots, x_{2n}\) als
Ortskoordinaten.
\end{example}
\label{vektoranalysis/multilinear:example-19}
\begin{example}{}{}



\par
Wir ordnen nun Vektoren
\(v = \begin{pmatrix} v_1\\ \vdots\\ v_n \end{pmatrix} =\sum_{k=1}^nv_ke_k\in\R^n\) verschiedene äußere Formen zu.
\begin{itemize}
\item {} 
\par
Das kanonische innere Produkt im \(\R^n\) vermittelt einen Isomorphismus

\end{itemize}
\begin{align*}
v\mapsto v^*,\ v^*(u) :=\, < v,u > \qquad(u\in\R^n)
\end{align*}
\par
des \(\R^n\) und seines Dualraumes. Die Eins–Form \(v^*\) besitzt dabei die Gestalt
\begin{align*}
v^* = \sum_{i=1}^nv_i\alpha_i
\in\Lambda^1(\R^n).
\end{align*}\begin{itemize}
\item {} 
\par
\(v\in\R^n\) wird auch eine \((n-1)\)–Form \(\omega_v\in\Lambda^{n-1}(\R^n)\),

\end{itemize}
\begin{align*}
\omega_v(u_2,\ldots,u_n) := \det(v,u_2,\ldots,u_n) \qquad (u_2,\ldots,u_n\in\R^n)
\end{align*}
\par
zugeordnet. Speziell im \(\R^3\) finden wir die \(2\)–Form
\begin{align*}
\omega_v = v_1\alpha_2\wedge\alpha_3+v_2\alpha_3\wedge\alpha_1+v_3
\alpha_1\wedge\alpha_2.
\end{align*}\begin{itemize}
\item {} 
\par
Wir betrachten jetzt speziell den (physikalisch wichtigen) \(\R^3\).
Das äußere Produkt zweier solcher \(1\)–Formen ergibtauf dem \(\R^3\) die \(2\)–Form

\end{itemize}
\begin{align*}
v^*\wedge u^* &=& (v_1\alpha_1+v_2\alpha_2+v_3\alpha_3)
\wedge(u_1\alpha_1+u_2\alpha_2+u_3\alpha_3)\\
&=& (v_1u_2-v_2u_1)\alpha_1\wedge\alpha_2+(v_2u_3-v_3u_2)\alpha_2\wedge
\alpha_3\\
&& + (v_3u_1-v_1u_3)\alpha_3\wedge\alpha_1\\
&=& \omega_{v\times u}.
\end{align*}
\par
Wir haben auf diese Weise das \href{https://de.wikipedia.org/wiki/Kreuzprodukt}{\emph{Kreuzprodukt}}
\begin{align*}
v\times u=\begin{pmatrix} v_2u_3-v_3u_2\\v_3u_1-v_1u_3 \\ v_1u_2-v_2u_1 \end{pmatrix} \in\R^3
\end{align*}
\par
zweier Vektoren \(v,u\in\R^3\) gewonnen.
\end{example}
\label{vektoranalysis/multilinear:theorem-20}
\begin{theorem}{}{}



\par
Die Vektoren \(w_1,\ldots,w_k\in E^*\) sind genau dann linear abhängig, wenn
\begin{align*}
w_1\wedge\ldots\wedge w_k=0.
\end{align*}\end{theorem}

\begin{proof}
 * Wenn sie linear abhängig sind, können wir einen Index \(i\in\{1,\ldots, k\}\) finden, für den \(w_i\) eine Linearkombination \(w_i=\sum_{\stackrel{l=1}{l\neq i}}^k c_l w_l\) ist. Damit gilt aber
\begin{align*}
w_1\wedge\ldots\wedge w_k = \sum_{\stackrel{l=1}{l\neq i}}^kc_l\, w_1 \wedge \ldots \wedge w_{i-1}\wedge w_l\wedge w_{i+1 \wedge\ldots\wedge w_k = 0,
\end{align*}
\par
denn in jedem Summanden kommt \(w_l\) doppelt vor.
\begin{itemize}
\item {} 
\par
Andernfalls können wir die Vektoren \(w_1,\ldots,w_k\) zu einer Basis

\end{itemize}
\begin{align*}
w_1,\ldots,w_n \text{ mit } n:=\dim(E^*)
\end{align*}
\par
ergänzen, sodass \(w_1\wedge\ldots\wedge w_n\neq0\) ist.
Dann ist aber auch \(w_1\wedge\ldots\wedge w_k\neq0\).
\end{proof}
\label{vektoranalysis/multilinear:Grassmannalgebra "uber $E$.}
\begin{definition}{}{}



\par
Für einen endlich dimensionalen \(\R\) Vektorraum \(E\) heißt der reelle Vektorraum
\begin{align*}
\Lambda^*(E) := \bigoplus_{k=0}^{\dim(E)}\Lambda^k(E)
\end{align*}
\par
(mit \(\Lambda^0(E):=\R\)) mit der durch das Dachprodukt
gegebenen Multiplikation die \textbf{äußere} oder
\href{https://de.wikipedia.org/wiki/Gra\%C3\%9Fmann-Algebra}{\textbf{Grassmann Algebra}}
\end{definition}
\label{vektoranalysis/multilinear:remark-22}
\begin{emphBox}{}{}{Remark 3.6}


\begin{itemize}
\item {} 
\par
\(\dim(\Lambda^*(E)) = 2^{\dim(E)}\), denn \(\sum_{k=0}^n{n\choose k} = 2^n\).

\item {} 
\par
Für beliebige \(k,l\in\N_0\) ist für alle \(\omega\in\Lambda^k(E)\) und
\(\varphi\in\Lambda^l(E)\):\textbackslash{} \(\omega\wedge\varphi\in\Lambda^{k+l}(E)\), aber für
\(m>\dim(E)\) ist \(\dim(\Lambda^m(E))=0\).

\end{itemize}
\end{emphBox}
\label{vektoranalysis/multilinear:pull-back von $\omega$ mit $f$.}
\begin{definition}{}{}



\par
Für eine lineare Abbildung \(f:E\to F\) endlichdimensionaler \(\R\)–Vektorräume und \(\omega\in\Lambda^k(F)\) heißt die durch
\begin{align*}
f^*(\omega)( v_1,\ldots, v_k) := \omega \big(f( v_1),\ldots,f( v_k)\big)
\qquad (v_1,\ldots,v_k\in E)
\end{align*}
\par
definierte \(k\)–Form \(f^*(\omega)\) die \textbf{Zurückziehung} (engl.
\textbf{pull–back}).
\end{definition}

\par
Es gilt offensichtlich \(f^*(\omega)\in\Lambda^k(E)\), denn \(f^*(\omega)\)
ist \(k\)–linear und antisymmetrisch.
\label{vektoranalysis/multilinear:theorem-24}
\begin{theorem}{}{}


\begin{itemize}
\item {} 
\par
Die Abbildung \(f^*:\Lambda^*(F)\to\Lambda^*(E)\) ist linear.

\item {} 
\par
Für \(g\in L(F,G)\) ist \((g\circ f)^*=f^*\circ g^*\).

\item {} 
\par
Für die identische Abbildung \(Id_E:E\to E\) ist \(Id_E^* = Id_{\Lambda^*(E)}\).

\item {} 
\par
Für eine invertierbare Abbildung \(f\in {\rm GL}(E,F)\) ist \((f^*)^{-1}=(f^{-1})^*\).

\item {} 
\par
\(f^*(\alpha\wedge\beta) = f^*(\alpha)\wedge f^*(\beta)\).

\end{itemize}
\end{theorem}

\begin{proof}
 Für alle Vektoren \(v_1,\ldots,v_k\in E\) gilt
\begin{itemize}
\item {} 
\par
Mit \(\alpha, \beta\in\Lambda^k(F)\) und \(c_1,c_2\in\R\) ist

\end{itemize}
\begin{align*}
f^*(c_1\alpha+c_2\beta)(v_1,\ldots,v_k)
 &=& (c_1\alpha+c_2\beta) \big(f(v_1),\ldots,f(v_k)\big)\\
&=& c_1\alpha\big(f(v_1),\ldots,f(v_k)\big) + c_2\beta\big(f(v_1),\ldots,f(v_k)\big)\\
&=& c_1f^*\alpha(v_1,\ldots,v_k)+c_2f^*\beta(v_1,\ldots,v_k).
\end{align*}\begin{itemize}
\item {} 
\par
\((g\circ f)^*\alpha( v_1,\ldots, v_k) = \alpha\big(g\circ f( v_1),\ldots, g\circ f( v_k)\big)= g^*\alpha\big(f( v_1),\ldots,f( v_k)\big)\\=  f^*\circ g^*\alpha( v_1,\ldots, v_k)\)

\item {} 
\par
\(Id_E^*(\alpha)(v_1,\ldots,v_k) = \alpha\big(Id_E(v_1),\ldots,Id_E(v_k)\big)
= \alpha(v_1,\ldots,v_k)\).

\item {} 
\par
Folgt aus 2. und 3.: \((f^{-1})^*f^* = (f\circ f^{-1})^* = Id_F^* =
Id_{\Lambda^*(F)}\).

\item {} 
\par
Hausaufgabe.

\end{itemize}
\end{proof}


\section{Tensoren und Tensorprodukte}
\label{\detokenize{vektoranalysis/tensor:tensoren-und-tensorprodukte}}\label{\detokenize{vektoranalysis/tensor::doc}}
\par
In diesem Kapitel widmen wir uns einem wichtigen aber komplizierten Thema der Vektoranalysis, nämlich Tensoren und Tensorprodukten.
Der Begriff hat sehr viele verschiedene Anschauungsmöglichkeiten (siehe \href{https://de.wikipedia.org/wiki/Tensorprodukt}{Wikipedia}) weshalb es nicht leicht ist eine Einführung zu geben die gleichzeitig allgemein, aber auch verständlich ist. Da Tensoren aber eine wichtige Rolle in der Physik spielen werden wir uns hier damit beschäftigen.


\subsection{Motivation}
\label{\detokenize{vektoranalysis/tensor:motivation}}
\par
Wir betrachten zwei Beispiele aus der Physik, welche auf Tensoren zurückgreifen.
\label{vektoranalysis/tensor:remark-0}
\begin{emphBox}{}{}{Remark 3.7}



\par
Der Begriff Tensor wurde von Hamilton in der Mitte des 19. Jahrhunderts eingeführt. Er leitete die Bezeichnung vom latinischen \emph{tendere} (spannen) ab, da die ursprüngliche Anwendung derartiger Objekte in der Elastizitätstheorie Anwendung fand.
\end{emphBox}


\subsubsection{Der Cauchy Spannungstensor}
\label{\detokenize{vektoranalysis/tensor:der-cauchy-spannungstensor}}
\begin{emphBox}{Augustin Cauchy}{}

\par
\href{https://de.wikipedia.org/wiki/Augustin-Louis\_Cauchy}{Augustin Louis Cauchy} (Geboren 21. August 1789 in Paris; Gestorben 23. Mai 1857 in Sceaux) war ein französischer Mathematiker.
\end{emphBox}

\par
Mechanische Spannung beschreibt die innere Beanspruchung und Kräfte in einem Volumen \(V\subset\R^3\) die aufgrund einer äußeren Belastungen auftreten. Die grundlegende Idee ist das \textbf{Euler Cauchy Stress Prinzip}, welches beschreibt, dass auf jede Schnittfläche \(A\subset\R^2\) welche ein Volumen in zwei Teile trennt, von diesen zwei Komponenten eine Spannung auf \(A\) ausgewirkt wird, welche durch den \textbf{Spannungsvektor} \(\mathbf{T}^n\) beschrieben wird. Der Spannungsvektor ist hierbei von der Dimension “Kraft pro Fläche”.

\begin{figure}[htbp]
\centering


\noindent\includegraphics[height=250\pxdimen]{{stress_vector}.png}
\caption{Visualisierung für Normal  und Scherspannung an einer Schnittfläche. Quelle: \href{https://en.wikipedia.org/wiki/Cauchy\_stress\_tensor}{Wikipedia; Cauchy Stress Tensor}.}\label{\detokenize{vektoranalysis/tensor:fig-stress}}\end{figure}

\par
Wie in \hyperref[\detokenize{vektoranalysis/tensor:fig-stress}]{Fig.\@ \ref{\detokenize{vektoranalysis/tensor:fig-stress}}} visualisiert teilt sich die Spannung in zwei Komponenten auf:

\par
\textbf{Normalspannung:}

\par
Dieser Teil des Spannungsvektor zeigt in Richtung der normalen \(\mathbf{n}\) welche orthogonal auf der Schnittfläche stehen.

\par
\textbf{Scherspannung:}

\par
Dieser Teil des Spannungstensors ist parallel zur Schnittfläche.

\par
Man erkennt nun, dass die Spannung in \(V\) nicht durch einen einzigen Vektor ausgedrückt werden kann. Einerseits hängt sie vom betrachteten Punkt \(P\in V\) ab und zudem von der Orientierung der Schnittfläche. Allerdings hat Cauchy gezeigt, dass ein Tensorfeld \(\mathbf{\sigma}(x)\) existiert, s.d.,
\begin{align*}
T^{\mathbf{n}}(x) = \mathbf{n}\cdot \mathbf{\sigma}(x),
\end{align*}
\par
d.h. in jedem Punkt \(x\in V\) ist der Stressvektor linear im Normalenvektor \(\mathbf{n}\).

\begin{figure}[htbp]
\centering


\noindent\includegraphics[height=250\pxdimen]{{stress_tensor_comp}.png}
\caption{Quelle: \href{https://de.wikipedia.org/wiki/Spannungstensor}{Wikipedia; Spannungstensor}.}\label{\detokenize{vektoranalysis/tensor:fig-stress-comp}}\end{figure}

\par
Hierfür betrachtet man einen freigeschnittenen Würfel wie in \hyperref[\detokenize{vektoranalysis/tensor:fig-stress-comp}]{Fig.\@ \ref{\detokenize{vektoranalysis/tensor:fig-stress-comp}}} und definiert für die drei verschiedenen Flächen (orthogonal zu den Einheitsvektoren) den Stresstensor
\begin{align*}
\mathbf{T}^{e_i}:= \sum_{j=1}^3 \sigma_{ij} e_j.
\end{align*}
\par
So haben wir z.B. für \(\mathbf{T}^{e_1}\) die Normalspannung gegeben durch \(\sigma_{11} e_1\) und die zwei Scherspannungskomponenten \(\sigma_{12} e_2, \sigma_{13} e_3\). Insgesamt erhält man neun Komponenten \(\sigma_{ij}\) welche über die Definition
\begin{align*}
\mathbf{\sigma} := \sum_{i=1}^3 e_i \otimes \mathbf{T}^{e_i} = \sum_{i=1}^3\sum_{j=1}^3 \sigma_{ij} (e_i\otimes e_j)
\end{align*}
\par
den Cauchy Stresstensor \(\mathbf{\sigma}\) ergebene. Hierbei bezeichnet \(\otimes\) das \textbf{dyadische Produkt} zweier Vektoren. Für \(x\in\R^n,y\in\R^m\) definieren wir
\begin{align*}
x \otimes y := 
\begin{pmatrix}
x_1y_1 &\ldots &x_1 y_m\\
\vdots &\ddots & \vdots\\
x_n y_1&\ldots& x_n y_m
\end{pmatrix}.
\end{align*}
\par
Wir werden später sehen, dass man die Idee \(\sigma\) über das dyadische Produkt zu definieren abstrahieren kann, was auf den allgemeinen Tensorbegriff führt.


\subsubsection{Quantenverschränkung}
\label{\detokenize{vektoranalysis/tensor:quantenverschrankung}}

\subsection{Das Tensorprodukt}
\label{\detokenize{vektoranalysis/tensor:das-tensorprodukt}}
\par
Wir wollen nun das Tensorprodukt von Vektorräumen abstrakt einführen und es später konkret realisieren.
\label{vektoranalysis/tensor:definition-1}
\begin{definition}{}{}



\par
Es seien \(V,W\) zwei reelle Vektorräume. Ein reeler Vektorraum \(X\) heißt \textbf{Tensorproduktraum} falls eine bilineare Abbildung \(\otimes:V\times W\rightarrow X\) existiert, s.d., die folgende \textbf{universelle Eigenschaft} gilt:

\par
Für jede Bilinearform \(\phi\in L^2(V\times W, Y)\) in einen beliebigen reellen Vektorraum \(Y\), existiert eine eindeutige lineare Abbildung
\(p \in L^1(X, Y)\), s.d. gilt
\begin{align*}
\phi(v,w) = p((v\otimes w)) = p(\otimes(v,w))\quad\forall (v,w)\in V\times W.
\end{align*}
\par
In diesem Fall schreibt man auch \(X = V\otimes X\), \(\otimes\) heißt Tensorprodukt und zusätzlich ist die Schreibweise \(\otimes(v,w)=:v\otimes w\) üblich.
\end{definition}

\par
\textbf{Was bedeutet das?}

\par
Diese Definition erscheint auf den ersten Blick abstrakt und unverständlich. Was ist jetzt also ein Tensorprodukt?

\par
\textbf{Das Tensorprodukt ist universell:}

\par
Wir haben benutzten in der Definition oben das kartesische Produkt \(\times\) welches eindeutig definiert ist. Im Gegensatz dazu gibt es nicht \emph{ein} Tensorprodukt \(\otimes\) oder \emph{einen} Tensorproduktraum \(V\otimes W\). Wir haben die Freiheit \(\otimes\) zu wählen und wann immer die universelle Eigenschaft erfüllt ist, heißt dann \(V\otimes W\) Tensorproduktraum. Derartige Konzepte nennt man in der Algebra \emph{universell}.

\par
\textbf{Was bedeutet die universelle Eigenschaft?}

\par
Wie wir weiter unten noch genauer beschreiben werden, stellt die universelle Eigenschaft eine wichtige Beziehung zwischen dem Raum der bilinearen Abbildungen auf \(V\times W\) und dem Dualraum von \(V\otimes W\) her. Sofern wir das Tensorprodukt gegeben haben erhalten wir alle Bilinearformen schon über einfache Linearformen auf \(V\otimes W\).


\subsection{Existenz und Konstruktion}
\label{\detokenize{vektoranalysis/tensor:existenz-und-konstruktion}}
\par
Wir können ein Tensorprodukt konkret konstruieren indem wir uns auf die Basis der Vektorräume \(V\) und \(W\) zurückziehen. Diese Tatsache formulieren wir in der folgenden Aussage.
\label{vektoranalysis/tensor:theorem-2}
\begin{theorem}{}{}



\par
Für zwei reelle Vektorräume \(V, W\) existiert stets mindestens ein Tensorprodukt \(\otimes\in L^2(V\times W, V\otimes W)\).
\end{theorem}

\begin{proof}
 Der folgende Beweis ist ein sogenannter konstruktiver Beweis, d.h., wir zeigen die Existenz eines Objekts indem wir es explizit angeben. Es gibt auch nicht konstruktive Existenzbeweise.

\par
Es sei \(B^V = \{b_i^V: i\in I^V\}\) eine Basis von \(V\) und analog \(B^W = \{b_i^W: i\in I^W\}\)  eine Basis von \(W\) für Indexmengen \(I^V, I^W\). Wir betrachten das kartesische Produkt
\begin{align*}
J := I^V \times I^W = \{(i,j): i\in I^V, j\in I^W\}.
\end{align*}
\par
Es sei nun \(X\) ein Vektorraum dessen Basis sich durch \(J\) indizieren lässt, d.h., es existiert eine Menge
\begin{align*}
B^X = \{b_{ij}^X: (i,j)\in J\}
\end{align*}
\par
s.d. \(B^X\) eine Basis von \(X\) ist. Ein solcher Vektorraum existiert, da z.B. das kartesische Produkt \(V\times W\) diese Eigenschaft erfüllt.

\par
Wir definieren nun eine bilineare Abbildung \(\otimes: V\times W\to X\) über
\begin{align*}
b_i^V \otimes b_j^W := b_{ij}^X\quad\forall (i,j)\in J.
\end{align*}
\par
Beachte, \(\otimes\) ist durch die Definition auf \(J\) eindeutig festgelegt, da für beliebige \((v,w)\in V\times W\) endlich viele Faktoren
\(\alpha_{i_1},\ldots,\alpha_{i_m}\) und \(\beta_{j_1},\ldots, \beta_{j_n}\) existieren s.d.
\begin{align*}
\otimes(v,w) 
&= 
\otimes\big(\sum_{k=1}^n \alpha_{i_k} b_{i_k}^V, \sum_{l=1}^m \beta_{j_l} b_{j_l}^W\big) 
\\&= 
\sum_{k=1}^n \sum_{l=1}^m \otimes\left(b_{i_k}^V, b_{j_l}^W\right)
\\&=
\sum_{k=1}^n \sum_{l=1}^m b_{i_kj_l}^X.
\end{align*}
\par
Wir müssen nun die universelle Eigenschaft zeigen, sei dazu \(\phi\in L^2(V\times W, Y)\) eine Bilinearform auf einen reellen Vektorraum \(Y\), dann können wir eine Linearform auf \(p:X\to Y\) definieren durch (analog reicht es die Definition auf den Basiselementen anzugeben)
\begin{align*}
p(b_{ij}^X) := \phi(b_i^V, b_j^W).
\end{align*}
\par
Dann gilt nämlich, unter Ausnutzung der Linearität von \(p\) und obiger Rechnung, dass
\begin{align*}
p(\otimes(v,w)) 
&=
\sum_{k=1}^n \sum_{l=1}^m p(b_{i_kj_l}^X)
\\&=
\sum_{k=1}^n \sum_{l=1}^m \phi\left(b_{i_k}^V, b_{j_l}^W\right)
\\&
\phi\big(\sum_{k=1}^n  b_{i_k}^V,\sum_{l=1}^m b_{j_l}^W\big)
\\&
\phi(v,w)
\end{align*}
\par
und somit gilt die universelle Eigenschaft. Insbesondere, da \(p\) durch die obige Definition eindeutig festgelegt ist.
\end{proof}

\par
Als Korollar erhalten wir somit, dass eine Basis des Tensorproduktraums durch das kartesische Produkt der ursprünglichen Basen konstruiert werden kann. Hieran sieht man qualitativ den Unterschied zwischen \(V\otimes W\) und \(V\otimes W\).
\label{vektoranalysis/tensor:corollary-3}
\begin{emphBox}{}{}{Corollary 3.1}



\par
Für zwei reelle Vektorräume \(V,W\) mit Basen \(B^V = \{b_i^V: i\in I^V\}, B^W = \{b_i^W: i\in I^W\}\) und ein Tensorprodukt \(\otimes:V\times W\to V\otimes W\) ist
\begin{align*}
\{b_i^V\otimes b_j^W: i\in I^V, j\in I^W\}
\end{align*}
\par
eine Basis von \(V\otimes W\).
\end{emphBox}

\par
Wir wissen nun, dass mindestens ein Tensorprodukt existiert, es stellt sich also die Frage inwiefern sich verschiedene derartige Abbildungen auf den gleichen Vektorräumen \(V,W\) unterschieden. Seien dazu \(\otimes_1, \otimes_2\) je zwei Tensorprodukte auf \(V\times W\). Wegen der universellen Eigenschaft gibt es lineare Abbildungen \(p_1: V\otimes_1 W\to W\otimes_2 V\) und \(p_2: V\otimes_2 W\to W\otimes_1 V\), s.d.,
\begin{align*}
\otimes_2 &= p_1 \circ \otimes_1\\
\otimes_1 &= p_2 \circ \otimes_2.
\end{align*}
\par
und somit
\begin{align*}
\otimes_2 &= p_1\circ p_2 \circ \otimes_2\\
\otimes_1 &= p_2\circ p_1 \circ \otimes_1.
\end{align*}
\par
Da wir aber die Basis von \(V\otimes_2 W\) über Elemente \(\otimes_2(b_i^V, b_j^W)\) charakterisieren können, und aus der ersten Gleichung folgt, dass
\begin{align*}
p_1\circ p_2(\otimes(b_i^V,b_j^W)) = \otimes(b_i^V, b_j^W)
\end{align*}
\par
wissen wir dass \(p_1\circ p_2 = \mathrm{Id}\). Das folgt da \(p_1\circ p_2\) als lineare Abbildung schon ganz auf den Basiselementen festgelegt ist.
Analog folgt \(p_2\circ p_1 = \mathrm{Id}\) und somit sind \(p_1, p_2\) isomorph zueinander. D.h. wir haben insgesamt gezeigt, dass verschiedene Tensorprodukte stets isomorph zueinander sind.
\label{vektoranalysis/tensor:lemma-4}
\begin{lemma}{}{}



\par
Es seien \(V,W\) zwei reelle Vektorräume und \(\otimes_1,\otimes_2\) zwei Tensorprodukte. Dann existiert genau ein Isomorphismus \(p:V\otimes_1 W\to V\otimes_2 W\), s.d.
\begin{align*}
\otimes_2 = p\circ \otimes_1.
\end{align*}\end{lemma}


\subsection{Tensoren als Linearformen}
\label{\detokenize{vektoranalysis/tensor:tensoren-als-linearformen}}
\par
Als Einleitung in das Thema wollen wir Tensoren zunächst als Linearformen auf \(\V_1\times\ldots\times\V_k\)
betrachten wobei für \(i=1,\ldots,k\) \(\V_i\) reelle endlich dimensionale Vektorräume sind.
Man schreibt in diesem Fall auch
\begin{align*}
\V_1\otimes\ldots\otimes\V_k = L(\V_1\times\ldots\V_k,\R)
\end{align*}
\par
wobei \(\otimes\) das Tensorprodukt bezeichnet.

\par
Der wichtige Spezialfall ist hier allerdings nun nicht \(\V^k\) sondern ein kartesisches Produkt der Form
\begin{align*}
(V^\ast)^r\times V^s.
\end{align*}\label{vektoranalysis/tensor:definition-5}
\begin{definition}{}{}



\par
Es sei \(\V\) ein reeller endlich dimensionaler Vektorraum, dann nennt man
\begin{align*}
T^r_s(V) := L((V^\ast)^r\times V^s, \R)
\end{align*}
\par
Menge der \(r\) fach \textbf{kontravarianten} und \(s\) fach \textbf{kovarianten} Tensoren, oder alternativ Tensoren der Stufe \((r,s)\).
\end{definition}

\par
Wir wollen diese abstrakte Definition nun mit einfachen Beispielen veranschaulichen zunächst für \(r+s=1\).
\label{vektoranalysis/tensor:example-6}
\begin{example}{}{}



\par
Tensoren der Stufe \((1,0)\) können mit Elementen des Vektorraums selbst identifiziert werden, denn
\begin{align*}
T^1_0(V) = L((V^\ast), \R) = \V^{\ast\ast}\cong \V
\end{align*}
\par
mit der Identifikation aus \cref{vektoranalysis/multilinear:lem:doubledual}  Weiterhin sind Tensoren der Stufe \((0,1)\) Elemente des
Dualraums, also einfach Linearformen auf \(\V\), sogenannte \emph{Kovektoren}.
\end{example}

\par
Als weiteren Spezialfall erhalten wir Multilinearformen.
\label{vektoranalysis/tensor:example-7}
\begin{example}{}{}



\par
Tensoren der Stufe \((0,k)\) sind \(k\) Linearformen, da \(T^0_k(V) = L^k(V)\).
\end{example}
\label{vektoranalysis/tensor:example-8}
\begin{example}{}{}



\par
Aus einer linearen Abbildung \(A:\V\to\V\) erhält man direkt einen Tensor der Stufe \((1,1)\) über die Abbildung
\begin{align*}
\varphi, v \mapsto \varphi(Av).
\end{align*}\end{example}


\section{Differentialformen}
\label{\detokenize{vektoranalysis/diffformen:differentialformen}}\label{\detokenize{vektoranalysis/diffformen::doc}}
\par
In diesem Kapitel werden wir nun \href{https://de.wikipedia.org/wiki/Differentialform}{Differentialformen} einführen. Die entscheidende Neuerung im Vergleich zum vorhergehenden Kapitel, ist
dass wir zusätzlich zur Vektorraumstruktur nun ein Konzept von Räumlichkeit einführen, speziell betrachten wir eine offene Menge \(U\subset\R^n\). Ein weiterer wichtiger Aspekt, ist dass wir im Folgenden mit glatten Funktion arbeiten wollen, d.h., mit dem Raum \(C^\infty(U,\R^n)\).

\par
Eine Differentialform \(\omega\) auf \(U\subseteq\R^n\) ist eine von Ort zu Ort variierende äußere Form, deren Variation wir als glatt voraussetzen.

\par
Wir schreiben eine allgemeine \emph{\(k\)–Form} \(\omega\) in der \emph{Grundform}
\begin{align*}
\omega = \sum_{1\leq i_1<\ldots<i_k\leq n}\omega_{i_1\ldots i_k}
dx_{i_1}\wedge\ldots\wedge dx_{i_k}\in\Omega^k(U),
\end{align*}
\par
wobei
\begin{itemize}
\item {} 
\par
die \(\omega_{i_1\ldots i_k}\in \Omega^0(U):=C^\infty(U,\R)\), also glatte reelle Funktionen auf \(U\) sind,

\item {} 
\par
und die \(dx_i\) den Koordinatenfunktionen \(x_i:\R^n\to\R\) zugeordnete \(1\)–Differentialformen sind (\(dx_i\in\Omega^1(\R^n)\)).

\item {} 
\par
Den Raum der \(k\)–Differentialformen schreiben wir ab jetzt zur Unterscheidung vom Raum der äußeren \(k\)–Formen mit dem Symbol \(\Omega\) statt \(\Lambda\).

\end{itemize}

\par
Die \(dx_i\) sind durch ihre Wirkung auf ein Vektorfeld \(v:U\to
\R^n\) definiert, und \(dx_i(v)( y) := v_i( y)\).
\(1\)–Differentialformen machen also aus Vektorfeldern Funktionen, und für \(k\) Vektorfelder \(v^{(l)}:U\to\R^n\) ist für das \(\omega\) aus der Grundform
\begin{align*}
\omega\left(v^{(1)},\ldots,v^{(k)}\right) := \sum_{1\leq i_1<\ldots<i_k\leq n}
\omega_{i_1\ldots i_k}\cdot\det\begin{pmatrix} dx_{i_1}(v^{(1)})&\ldots& dx_{i_k}(v^{(1)})\\
\vdots&&\vdots\\
dx_{i_1}(v^{(k)})&\ldots& dx_{i_k}(v^{(k)}) \end{pmatrix}
\end{align*}
\par
definiert. Das Ergebnis ist also eine reelle Funktion auf \(U\).\textbackslash{}
Die Rechenregeln übertragen sich von den äußeren Formen auf die Differentialformen.

\par
Auf dem \(\R\)–Vektorraum
\begin{align*}
\Omega^*(U) := \bigoplus_{k=0}^n\Omega^k(U)
\end{align*}
\par
der Differentialformen betrachten wir jetzt
den \emph{Differentialoperator} \(d\), der durch
\begin{itemize}
\item {} 
\par
\(df := \sum_{i=1}^n\frac{\partial f}{\partial x_i}dx_i\) für Funktionen
\(f\in C^\infty(U,\R) = \Omega^0(U)\)

\item {} 
\par
und \(d\omega := \sum_{1\leq i_1<\ldots<i_k\leq n}d\omega_{i_1\ldots i_k}
\wedge dx_{i_1}\wedge\ldots\wedge dx_{i_k}\) für \(k\)–Formen \textbackslash{}linebreak
\(\omega = \sum_{1\leq i_1<\ldots<i_k\leq n}\omega_{i_1\ldots i_k}
dx_1\wedge\ldots\wedge dx_{i_k}\)

\end{itemize}

\par
definiert ist. \(d\) verwandelt eine \(k\)–Form also in eine \((k+1)\)–Form.
\label{vektoranalysis/diffformen:aeussere Ableitung}
\begin{definition}{}{}



\par
Die lineare Abbildung \(d:\Omega^*(U)\to\Omega^*(U)\) heißt \href{https://de.wikipedia.org/wiki/\%C3\%84u\%C3\%9Fere\_Ableitung}{\textbf{äußere Ableitung}}.
\end{definition}
\label{vektoranalysis/diffformen:ex:10.14}
\begin{example}{}{}


\begin{enumerate}

\item {} 
\par
Für \(\omega\in\Omega^0(\R^3)\) ist \(d\omega = \frac{\partial\omega}{\partial x_1}dx_1+
\frac{\partial\omega}{\partial x_2}dx_2+\frac{\partial\omega}{\partial x_3}dx_3\).

\item {} 
\par
Für \(\omega = \omega_1dx_1+\omega_2dx_2+\omega_3dx_3\in\Omega^1(\R^3)\) ist

\end{enumerate}
\begin{align*}
d\omega &=& (d\omega_1)\wedge dx_1+(d\omega_2)\wedge dx_2+(d\omega_3)\wedge
dx_3\\
&=& \left(\frac{\partial\omega_2}{\partial x_1}-\frac{\partial\omega_1}{\partial x_2}\right)
dx_1\wedge dx_2+ \left(\frac{\partial\omega_3}{\partial x_2}-\frac{\partial\omega_2}{\partial x_3}\right)
dx_2\wedge dx_3\\
&& + \left(\frac{\partial\omega_1}{\partial x_3}-\frac{\partial\omega_3}{\partial x_1}\right)
dx_3\wedge dx_1
\end{align*}\begin{enumerate}

\item {} 
\par
Für \(\omega = \omega_{12}dx_1\wedge dx_2+\omega_{23}dx_2\wedge dx_3
+\omega_{31}dx_3\wedge dx_1 \in\Omega^2(\R^3)\) ist

\end{enumerate}
\begin{align*}
d\omega = \left(\frac{\partial\omega_{12}}{\partial x_3} + \frac{\partial\omega_{23}}{\partial x_1}
+ \frac{\partial\omega_{31}}{\partial x_2}\right)dx_1\wedge dx_2\wedge dx_3.
\end{align*}\begin{enumerate}

\item {} 
\par
Für \(\omega\in\Omega^3(\R^3)\) ist \(d\omega=0\).

\end{enumerate}
\end{example}
\label{vektoranalysis/diffformen:Antiderivation}
\begin{theorem}{}{}



\par
\(d\) ist eine \href{https://de.wikipedia.org/wiki/Derivation\_(Mathematik)\#Antiderivationen}{\textbf{Antiderivation}}, d.h. für \(\alpha\in\Omega^k(U)\) und \(\beta\in\Omega^l(U)\) ist
\begin{align*}
d(\alpha\wedge\beta) = (d\alpha)\wedge\beta+(-1)^k\alpha\wedge d\beta.
\end{align*}\end{theorem}

\begin{proof}
 Wegen der Linearität von \(d\) genügt es, diese Gleichung für Monome
\begin{align*}
\alpha := f\underbrace{dx_{i_1}\wedge\ldots\wedge dx_{i_k}}_{\tilde
{\alpha}},\ \beta := g\underbrace{dx_{j_1}\wedge\ldots\wedge dx_{j_l}}_
{\tilde{\beta}},\ f,g\in C^\infty(U,\R)
\end{align*}
\par
zu beweisen.
Es gilt
\begin{align*}
d(\alpha\wedge\beta) &=& d(f\cdot g)\tilde{\alpha}\wedge
\tilde{\beta} = \big((df)g+f(dg)\big)\,\tilde{\alpha}\wedge\tilde{\beta}\\
&=& (df)\tilde{\alpha}\wedge g\tilde{\beta}+ (-1)^kf\tilde{\alpha}
\end{align*}\end{proof}
\label{vektoranalysis/diffformen:thm:dd}
\begin{theorem}{}{}



\par
Auf \(\Omega^*(U)\) gilt
\end{theorem}

\begin{proof}
 1. Für \(f\in\Omega^0(U)\) ist
\begin{align*}
ddf &=& d\left(\sum_{i=1}^n\frac{\partial f}
{\partial x_i}dx_i\right) = \sum_{i=1}^n\sum_{l=1}^n\frac{\partial^2f}{\partial x_l\partial x_i}
dx_l\wedge dx_i\\
& =& \sum_{1\leq r< s\leq n}\left(\frac{\partial^2 f}{\partial x_r
\partial x_s} - \frac{\partial^2f}{\partial x_s\partial x_r}\right)dx_r\wedge dx_s = 0,
\end{align*}
\par
da wir wegen der Glattheit von \(f\) die partiellen Ableitungen vertauschen
können.
\begin{enumerate}

\item {} 
\par
Für \(\omega = \sum\omega_{i_1\ldots i_k}dx_{i_1}\wedge\ldots\wedge dx_{i_k}
\in\Omega^k(U)\) ist\textbackslash{}

\end{enumerate}
\begin{align*}
dd\omega = \sum(\underbrace{dd\omega_{i_1\ldots i_k}}_0)
\wedge dx_{i_1}\wedge\ldots\wedge dx_{i_k} = 0,
\end{align*}
\par
denn gemäß Satz \cref{vektoranalysis/diffformen:Antiderivation} wird die äußere Ableitung auf die
1 Formen \(d\omega_{i_1\ldots i_k}\) und \(dx_{i_l}\) angewandt, und nach Teil 1.
ist das Ergebnis Null.
\end{proof}
\label{vektoranalysis/diffformen:geschlossen:exakt}
\begin{definition}{}{}



\par
Eine Differentialform \(\vv\in\Omega^*(U)\) heißt
\begin{itemize}
\item {} 
\par
\textbf{geschlossen}, wenn \(d\vv=0\), *\textbf{exakt}, wenn \(\vv=d\psi\) für ein \(\psi\in\Omega^*(U)\) gilt.

\end{itemize}

\par
Nach Satz \cref{vektoranalysis/diffformen:thm:dd} sind exakte Differentialformen geschlossen.\textbackslash{} Für \(k\)–Formen auf konvexen offenen Teimengen \(U\subseteq \R^n\) gilt für \(k\ge 1\)auch die Umkehrung (sog.
\href{https://de.wikipedia.org/wiki/Poincar\%c3\%a9-Lemma}{\textbf{Poincaré Lemma}} ),  siehe Kapitel \code{\upquote{sect:Poinca}}).
\end{definition}


